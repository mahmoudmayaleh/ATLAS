
================================================================================
COMPREHENSIVE RESEARCH ANALYSIS: ATLAS PROJECT PAPERS
================================================================================
Date: December 23, 2025
Workspace: c:\Users\Hp\Downloads\Advanced_project\ATLAS

Papers Analyzed:
1. ATLAS_Base_Specification.pdf (1 page)
2. ATLAS_V1.pdf (22 pages) 
3. HSplitLoRA.pdf (16 pages)
4. MIRA_A_Method_of_Federated_Multi-Task_Learning_for_Large_Language_Models.pdf (5 pages)
5. Privacy-Aware_Split_Federated_Learning_for_LLM_Fine-Tuning_over_Internet_of_Things.pdf (12 pages)
6. SplitLoRA.pdf (9 pages)
7. VFLAIR-LLM.pdf (12 pages)

================================================================================

================================================================================
PAPER: ATLAS_Base_Specification.pdf
Total Pages: 1
================================================================================

EXTRACTED CONTENT (First 5000 characters):
Conservatoire national des arts et métiers Mahmoud Mayaleh
US PROJ 2025/2026 Project # 13
Lyes Khoukhi & Zakria Abouelhouda
Implementation and evaluation of efficient methods for fine-tuning large-scale language
models (LLM) in a federated learning framework
Base Specification
The widespread adoption of large language models has transformed natural language processing, yet their
deployment on edge devices faces two critical challenges. Models with billions of parameters exceed the
memory and computational capacity of individual devices, requiring collaborative training across multiple
nodes. Additionally, clients operate on heterogeneous task-specific data such as medical records, financial
transactions, or conversational logs, making naive parameter sharing inefficient or harmful. Existing
solutions address only one challenge: federated multi-task learning frameworks like MIRA provide task
awareness through centralized coordination but cannot split models across devices, while peer-to-peer
split learning approaches like TITANIC enable model partitioning but remain task-blind under data
heterogeneity.
The primary objective of this project is to design, implement, and evaluate ATLAS, a self-organizing
decentralized multi-task learning framework for large language model fine-tuning on heterogeneous edge
devices. ATLAS combines gradient-based semantic client clustering with peer-to-peer split learning and
cluster-wise adapter aggregation to enable collaborative fine-tuning without centralizing raw data or
requiring any device to host the entire model. The framework automatically discovers task-aware
semantic neighborhoods of clients with similar workloads and exploits these neighborhoods for efficient
routing and aggregation.
The first phase involves comprehensive analysis of federated multi-task learning, split learning, and
parameter-efficient adaptation techniques. This includes studying how MIRA leverages centralized task
graphs while identifying its bottlenecks, examining TITANIC's peer-to-peer architecture and its
limitations under task heterogeneity, and reviewing adapter-based methods like LoRA for decentralized
aggregation. The outcome is an architectural specification defining task embeddings, clustering
procedures, resource-aware pairing constraints, and hierarchical aggregation mechanisms.
The second phase focuses on implementing a simulation environment instantiating the ATLAS lifecycle.
Clients compute gradient-based task embeddings from local data and send compact signatures to a server
that performs clustering to identify semantic neighborhoods. The server initializes shared LoRA adapters
and returns peer partnership information. During training, clients execute split learning by partitioning
model layers according to memory constraints and exchange intermediate activations directly. Robustness
mechanisms including intra-cluster re-pairing and inter-cluster fallback handle stragglers and client
dropouts.
The final phase conducts systematic experimental evaluation measuring task-specific accuracy,
communication volume, convergence speed, computational overhead, and semantic clustering quality.
ATLAS will be compared against centralized training, federated multi-task baselines, and task-blind split
learning to characterize the benefits of combining semantic routing with peer-to-peer model partitioning
and derive actionable deployment guidelines.


[... Content continues ...]


================================================================================
PAPER: ATLAS_V1.pdf
Total Pages: 22
================================================================================

EXTRACTED CONTENT (First 5000 characters):
ATLAS
Adaptive Task-aware Layered Aggregation for Split Learning
Your Name
December 23, 2025
Why ATLAS? - Challenge 1: Model Size
LLMs are too large for edge devices
Modern models: 7B to 70B parameters
Far exceed memory constraints of:
Mobile phones
IoT devices
Edge servers
Problem: Inference and training are infeasible on weak devices
YourName ATLAS December23,2025 2/22
Why ATLAS? - Challenges & Device Heterogeneity
Challenge 2: Heterogeneity
GPU
Different device budgets (1GB 24GB Strong:
fullyused
to 24GB GPU) Client A
Different task types (NLP, GPU
Q&A, summarization) 8GB Medium:
balanced
Client B
Different network speeds (5G,
4G, WiFi)
GPU
1GB Weak:
ClientC limited
Challenge 3: Efficiency
One-size-fits-all approach
Wastes resources on weak
devices
Underutilizes strong devices
YourName ATLAS December23,2025 3/22
Standing on Existing Work: SplitLoRA
SplitLoRA
Split model between client and server
Trainable LoRA adapters for efficiency
Homogeneous setup (all clients identical)
Advantage: Efficient communication
Limitation: No heterogeneity support
YourName ATLAS December23,2025 4/22
Standing on Existing Work: HSplitLoRA
HSplitLoRA
Heterogeneous LoRA ranks per client
Adapted to device budgets
Different configurations per device
Advantage: Efficient + heterogeneous
Limitation: No task awareness (ignores task similarity)
YourName ATLAS December23,2025 5/22
Standing on Existing Work: MIRA
MIRA
Task clustering for small models
Clients in same cluster share configurations
Knowledge sharing between similar tasks
Advantage: Efficient + task-aware
Limitation: Not designed for split learning with LLMs
YourName ATLAS December23,2025 6/22
Comparison: ATLAS vs. Existing Methods
Method Heterogeneous Task-Aware Efficient
SplitLoRA No No Yes
HSplitLoRA Yes No Yes
MIRA Yes Yes No
ATLAS Yes Yes Yes
ATLAS = Combining all three ideas for LLMs
YourName ATLAS December23,2025 7/22
ATLAS: System Overview
ATLAS operates in four phases:
1 Phase 1: Discover similar tasks through clustering
2 Phase 2: Configure device-specific LoRA ranks
3 Phase 3: Train using split learning
4 Phase 4: Aggregate updates per cluster
Result: Federated LLM training on heterogeneous devices with task
awareness
YourName ATLAS December23,2025 8/22
Phase 1: Discover Similar Tasks - Goal
Goal: Group clients with similar tasks to enable knowledge sharing
Process:
1 Each client computes gradients on its own data (small batch)
2 Compress gradients into 64-dimensional “task fingerprint”
3 Server collects all fingerprints
4 k-Means clustering groups similar tasks
Privacy: Only low-dim vectors sent; no raw data or full gradients
YourName ATLAS December23,2025 9/22
Clients 1,2; [ellipse, draw, fill=green!20, minimum width=1.5cm,
right=2.5cm of f3] (cl2) Cluster 2
Client 3;
[-¿, thick] (c1) – (f1); [-¿, thick] (c2) – (f2); [-¿, thick] (c3) – (f3); [-¿,
thick] (f1) – (cl1); [-¿, thick] (f2) – (cl1); [-¿, thick] (f3) – (cl2);
YourName ATLAS December23,2025 10/22
Phase 2: Assign Heterogeneous Configurations
Goal: Choose LoRA rank and split point for each client
Process:
1 Determine weight importance per task cluster
2 Assign higher ranks to important weights
3 Respect each client’s memory budget
4 Similar rank patterns within clusters, different absolute values
Example:
Client A (24GB): ranks [8, 6, 4]
Client B (8GB): ranks [6, 4, 2]
Client C (1GB): ranks [4, 2, 1]
YourName ATLAS December23,2025 11/22
Phase 3: Train with Split Learning
Goal: Train LoRA adapters using client-server split architecture
Architecture:
Client: Input processing + early layers + LoRA adapters
Server: Task layers + loss head + LoRA adapters
Per round:
1 Client sends intermediate activations h to server
2 Server computes loss and gradients ∇h
3 Server sends gradients back to client
4 Both update LoRA adapters using local SGD
Key: No full model or gradients transmitted; minimal communication
YourName ATLAS December23,2025 12/22
Phase 4: Aggregate Updates Per Cluster
Goal: Merge LoRA adapters from same cluster for global model
Key Innovation: Noise-Free Aggregation
Even with different ranks, we can merge perfectly:
Client 1: rank 4, Client 2: rank 6, Client 3: rank 4
Concatenate low-rank factors ⇒ rank 14
Merge using aggregation formula
Result: quality of averaging, no information loss
Process:
1 Stack all client adapters
2 Apply weighted average in merged space
3 Broadcast updated adapter back
4 Clients decompose for next round
YourName ATLAS December23,2025 13/22
ATLAS Advantages: Efficiency (30-40% Memory Saved)
Weak devices get small LoRA ranks ⇒ fit within budget
Strong devices use larger ranks ⇒ utilize resources
Split learning minimizes communication overhead
Heterogeneous config prevents over-provisioning
Result: Significant memory savings while maintaining performance
YourName ATLAS December23,2025 14/22
ATLAS Advantages: Accuracy (20-30% Gain)
Task clustering enables knowledge transfer
Clients specialize within clusters
Shared cluster-level model provides regularization
LoRA ranks ensure

[... Content continues ...]


================================================================================
PAPER: HSplitLoRA.pdf
Total Pages: 16
================================================================================

EXTRACTED CONTENT (First 5000 characters):
1
HSplitLoRA: A Heterogeneous Split
Parameter-Efficient Fine-Tuning Framework for
Large Language Models
Zheng Lin, Yuxin Zhang, Zhe Chen, Member, IEEE, Zihan Fang, Xianhao Chen, Member, IEEE, Praneeth
Vepakomma, Member, IEEE, Wei Ni, Fellow, IEEE, Jun Luo, Fellow, IEEE, and Yue Gao, Fellow, IEEE
Abstract—Recently, large language models (LLMs) have in both industry and academia. These models excel in ex-
achieved remarkable breakthroughs, revolutionizing the natural tracting intricate patterns from vast datasets and adapting
languageprocessingdomainandbeyond.Duetoimmenseparam-
to diverse downstream tasks, ranging from natural language
eter sizes, fine-tuning these models with private data for diverse
understanding and generation to specialized domains such
downstream tasks has become mainstream. Though federated
learning (FL) offers a promising solution for fine-tuning LLMs as smart healthcare [6], [7], finance [8], and intelligent
withoutsharingrawdata,substantialcomputingcostshinderits transportation [9], [10]. However, due to immense parameter
democratization.Moreover,inreal-worldscenarios,privateclient sizes, training LLMs from scratch is typically infeasible [11],
devicesoftenpossessheterogeneouscomputingresources,further
rendering fine-tuning with private data for specific down-
complicating LLM fine-tuning. To combat these challenges, we
streamtasksbecomemainstream.Theworkflowoffine-tuning
propose HSplitLoRA, a heterogeneous parameter-efficient fine-
tuning (PEFT) framework built on split learning (SL) and low- LLMs typically follows a three-stage process: i) pre-training
rank adaptation (LoRA) fine-tuning, for efficiently fine-tuning LLMsfromscratchonextensivetextcorpora(e.g.,Wikipedia)
LLMsonheterogeneousclientdevices.HSplitLoRAfirstidentifies using substantial computing resources (e.g., training GPT3-
importantweightsbasedontheircontributionstoLLMtraining.
1.3B model requires 64 Tesla V100 GPUs running for one
ItthendynamicallyconfiguresthedecompositionranksofLoRA
week [12]); ii) fine-tuning pre-trained LLMs to adapt to
adapters for selected weights and determines the model split
point according to varying computing budgets of client devices. specificdownstreamtasks;iii)deployingthefine-tunedLLMs
Finally,anoise-freeadapteraggregationmechanismisdevisedto on client devices to perform inference.
support heterogeneous adapter aggregation without introducing Unfortunately, privacy concerns often hinder the collabora-
noise. Extensive experiments demonstrate that HSplitLoRA out-
tive fine-tuning of LLMs across multiple parties, even when
performs state-of-the-art benchmarks in training accuracy and
theysharethesamedownstreamtaskandcouldmutuallybene-
convergence speed.
fit.Forexample,severalfreelancefinancialanalystsmaywish
Index Terms—Distributed learning, split learning, large lan-
tocollaborativelyfine-tuneanLLMforaspecialfinancialrisk
guage model, parameter-efficient fine-tuning.
predictiontaskviatheirpersonaldevices,buttheyarereluctant
I. INTRODUCTION to share customer data [8]. Federated learning (FL) [13],
[14] offers a promising solution to this privacy issue [15],
Recently, large language models (LLMs) have achieved
[16] by enabling participants to fine-tune LLMs using local
tremendoussuccessacrossabroadspectrumofpivotalsectors
data without sharing the raw data [17], [18]. However, fine-
due to their exceptional ability in handling high-complexity
tuning LLMs via FL is non-trivial, particularly for resource-
and large-scale datasets [1]–[5]. Notable examples include
constrained client devices. In typical configurations, client
OpenAI’s GPT series [1], Google’s Gemini [2], and Meta’s
devices are equipped with low-cost commercial GPUs, such
LLaMA [3], all of which have gained significant attention
as the NVIDIA GeForce RTX 3050 in a laptop. While these
GPUs are sufficient for conventional deep learning tasks such
Z. Lin, Y. Zhang, Z. Chen, Z. Fang, and Y. Gao are with the Institute
as object detection [19] and image classification [20], they
of Space Internet, Fudan University, Shanghai 200438, China, and the
SchoolofComputerScience,FudanUniversity,Shanghai200438,China(e- can be overwhelmed by the immense computing workload
mail:zlin20@fudan.edu.cn;zhfang19@fudan.edu.cn;zhechen@fudan.edu.cn; of full parameter fine-tuning of LLMs within standard FL
yxzhang24@m.fudan.edu.cn;gao.yue@fudan.edu.cn).Z.Linisalsowiththe
frameworks. To combat this issue, some recent studies [21],
Department of Electrical and Electronic Engineering, University of Hong
Kong,PokFuLam,HongKong,China. [22] focus on integrating FL with parameter-efficient fine-
X. Chen is with the Department of Electrical and Electronic Engineer- tuning (PEFT) methods, such as Adapter [23] and low-rank
ing, University of Hong Kong, Pok Fu Lam, Hong Kong, China (e-mail:
adaptation (LoRA) [11]. Despite these efforts, fine-tuning
xchen@eee.hku.hk).
P.VepakommaiswithMohamedbinZayedUniversityofArtificialIntelli- LLMs via FL on client devices remains p

[... Content continues ...]


================================================================================
PAPER: MIRA_A_Method_of_Federated_Multi-Task_Learning_for_Large_Language_Models.pdf
Total Pages: 5
================================================================================

EXTRACTED CONTENT (First 5000 characters):
IEEENETWORKINGLETTERS,VOL.7,NO.3,SEPTEMBER2025 171
MIRA: A Method of Federated Multi-Task Learning for Large Language Models
Ahmed Elbakary , Graduate Student Member, IEEE, Chaouki Ben Issaid , Member, IEEE, Tamer ElBatt,
Karim Seddik , Senior Member, IEEE, and Mehdi Bennis , Fellow, IEEE
Abstract—Inthisletter,weintroduceamethodforfine-tuning jointly to solve a specific problem by leveraging local data
LargeLanguageModels(LLMs),inspiredbyMulti-Tasklearning for local training and sharing only the model’s parameters,
in a federated manner. Our approach leverages the structure
insteadofthelocaldata.TraditionalFLalgorithmsaimtotrain
of each client’s model and enables a learning scheme that
a global model that can be deployed across all participating
considers other clients’ tasks and data distribution. To mitigate
clients, regardless of the underlying distribution of their data.
the extensive computational and communication overhead often
associatedwithLLMs,weutilizeaparameter-efficientfine-tuning Multiple frameworks already exist to leverage FL for fine-
method,specificallyLow-RankAdaptation(LoRA),toreducethe tuningLLMs.Forexample,Zhangetal.[4]proposedtheusage
number of trainable parameters. Experimental results, with dif- of different Parameter-efficient fine-tuning (PEFT) methods
ferent datasets and models, demonstrate the proposed method’s
like prompt tuning to make the fine-tuning process of LLMs
effectivenesscomparedtoexistingframeworksforfederatedfine-
communicationefficient.Theideaofprompttuningistoadda
tuning of LLMs in terms of global and local performances. The
proposed scheme outperforms existing baselines by achieving setofvirtualtokens,oftencalledasoftprompt,totheoriginal
lower local loss for each client, while maintaining comparable prompt,i.e.,thehardprompt.Then,ittriestooptimizethesoft
global performance. prompttogeneratethedesiredoutput.In[5],theauthorsmade
Index Terms—Large language models, federated learning, useofthefactthatmostFLsettingsoperatewithedgedevices
multi-task learning, deep neural networks. designed for inference not training. This characteristic leads
to solving the problem using a zeroth-order method without
the need for backpropagation (BP).
I. INTRODUCTION
The main problem with the existing federated fine-tuning
PRE-TRAINED Large Language Models (LLMs) are approaches for LLMs is the fact that they all learn a single
proventobefew-shotlearners[1],whichmeansthatthey global model based on model averaging, which might not
can perform a diverse set of tasks without retraining on those be the optimal solution for highly heterogeneous settings.
specifictasks.Thispropertyisattributedmainlytotwofactors: Moreover, a single global model might not be the optimal
the wide range of their training data and the scale of the solution when the model is expected to perform well across
architecture.Whilein-contextlearning[2]mightbeenoughto different tasks where the data distribution differs significantly.
adapt the LLM to the task at hand, this might not be possible For example, different medical entities around the world
without further model fine-tuning. The problem arises when might train a chatbot system to interact with their patients.
adapting these models to a domain-specific area where the A single global model may fail to capture the nuanced med-
data comes from a different distribution than the training data ical terminologies, treatment protocols, or diagnostic criteria
itself. This requires further fine-tuning of the models so they uniquetoeachregionorinstitution,thusreducingthemodel’s
can perform at a reasonable level. Gathering such domain- effectiveness in real-world clinical settings. Federated Multi-
specific data might be impractical because of the associated Task learning (FMTL) [6] provides a promising framework
cost of data collection in addition to privacy concerns that that can be leveraged to solve the issue of heterogeneous
might arise. clients. Several approaches have been proposed to adapt MTL
Federated learning (FL) [3] is one technique that can be tomodelFLproblems.Forexample,in[7],theauthorsaligned
leveraged to solve such a problem without revealing local the locally learned model and the global one using regular-
data. The idea is to enable different clients to train a model ization, improving both model personalization and fairness
amongclients.Theexistenceofahiddenrelationshipbetween
Received 7 November 2024; revised 8 January 2025; accepted 31
clients’ models was assumed in [8], and the Laplacian matrix
January 2025. Date of publication 7 February 2025; date of current version
14October2025.ThisworkwassupportedbytheEuropeanUnionthrough was leveraged to capture this relationship.
the Project 6G-INTENSE under Grant 101139266. The associate editor The second issue is concerned with the computation and
coordinatingthereviewofthisarticleandapprovingitforpublicationwasJ.
communication costs associated with LLMs. An LLM is, at
Wang.(Correspondingauthor:AhmedElbakary.)

[... Content continues ...]


================================================================================
PAPER: Privacy-Aware_Split_Federated_Learning_for_LLM_Fine-Tuning_over_Internet_of_Things (1).pdf
Total Pages: 12
================================================================================

EXTRACTED CONTENT (First 5000 characters):
This article has been accepted for publication in IEEE Internet of Things Journal. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/JIOT.2025.3600269
1
Privacy-Aware Split Federated Learning for LLM
Fine-Tuning over Internet of Things
Xiaopei Chen, Student Member, IEEE, Wen Wu, Senior Member, IEEE, Fei Ji, Member, IEEE,
Yongguang Lu, and Liang Li, Member, IEEE
Abstract—The proliferation of Internet of Things (IoT)- billion[2],generatingrich,real-time,anddomain-specificdata
generated distributed personal data enables user-specific large streams [3]. These IoT devices offer rich and diverse data
languagemodel(LLM)adaptationattheedge.Thesplitfederated
to support LLM development, while LLMs are expected to
learning (SFL) facilitates collaborative learning and reduces
enhance the intelligence of IoT [4–6]. This convergence of
memory footprint by model splitting, which necessitates the
transmission of intermediate activations, rendering it suscep- IoT and LLM technologies thus creates unprecedented oppor-
tible to reconstruction attacks and privacy breaches. In this tunities to advance smart services and connected applications.
paper, we present a privacy-aware SFL scheme addressing the As IoT devices persistently generate decentralized data with
accuracy-efficiency-privacy trilemma in LLM fine-tuning over
privacy implications [7, 8], there is a critical requirement to
heterogeneous IoT devices. Particularly, we develop a privacy
adapt LLMs through localized fine-tuning to meet specialized
quantification metric based on Fisher information to assess
layer-wise privacy risks in smashed data transmission. Guided service needs while ensuring information privacy.
by this metric, we establish an analytical model that captures Split federated learning (SFL) offers an efficient frame-
the intricate relationships between privacy leakage, fine-tuning work for fine-tuning LLMs in IoT networks by harnessing
convergence time, and device energy consumption. To optimize
distributed device data while overcoming memory limitations
thesethreeaspects,weformulateamulti-objectivemixed-integer
[9, 10]. Unlike conventional federated learning (FL) that
programming problem. Then, an (cid:15)-constraint-based block coor-
dinatedescent(BCD)algorithmisproposedtojointlydetermine requires storing the whole model and processing on each
the optimal LLM split layer, transmit power, and bandwidth device [11], SFL divides the LLM into client-side and server-
allocationforIoTdevicesundertheirmemoryandnetworkcon- side submodels [12]. Each IoT device exclusively trains its
straints. Extensive simulation results demonstrate the proposed
assignedclientsubmodel,dramaticallyreducinglocalmemory
scheme’seffectivenessinachieving24%fasterconvergence,40%
and processing requiremets, making it particularly suitable
lower energy consumption, and 7% reduced privacy leakage
comparedtobaselineapproaches,whilemaintainingcompetitive for resource-constrained IoT devices. In the SFL scheme, the
model accuracy. devices locally update their submodels and the edge server
updates the corresponding submodels, where the intermediate
Index Terms—LLM fine-tuning, split federated learning, pri-
vacy leakage, multi-objective optimization. outputs (smashed data) and the gradients at the split layer
are exchanged between the device and the server. A critical
challenge lies in optimizing model split strategies to accom-
I. INTRODUCTION modate devices with varying computational capacities and
The rapid advancement of artificial intelligence (AI) has memory resources. This directly impacts resource efficiency,
driven large language models (LLMs), such as GPT and fine-tunedmodelaccuracy,andthesystem’sabilitytomitigate
LLaMa, to achieve remarkable progress in natural language performance bottlenecks caused by slower devices.
processingandintelligentdecision-making.Thesemodelsrely In the literature, several pioneering studies [13–18] have
on large-scale, diverse data for continual improvement [1]. explored integrating SFL with parameter-efficient fine-tuning
Meanwhile, with the rapid growth of the Internet of Things (PEFT)methodslikelow-rankadaptation(LoRA)[19],where
(IoT),thenumberofglobalIoTdevicesisexpectedtobe3.56 only minimal activated parameters (rather than full model
parameters) undergo updates during training. Existing works
This work was supported in part by the Pengcheng Laboratory Major primarily focus on optimizing training latency [13–15] and
Key Project under Grant 2025QYA002 and 2025QYB041, in part by the energy consumption [15–17] through strategic split layer
NaturalScienceFoundationofChinaunderGrant62201071,62201311,and
selection, computation resource scheduling, and communi-
62192712.(Correspondingauthor:WenWu.)
Xiaopei Chen is with the School of Future Technology, South China cation resource allocation. The core challenge resides in
University of Technology, Guangzhou 51

[... Content continues ...]


================================================================================
PAPER: SplitLoRA.pdf
Total Pages: 9
================================================================================

EXTRACTED CONTENT (First 5000 characters):
JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2015 1
SplitLoRA: A Split Parameter-Efficient Fine-Tuning
Framework for Large Language Models
Zheng Lin, Xuanjie Hu, Yuxin Zhang, Zhe Chen, Member, IEEE, Zihan Fang, Xianhao Chen, Member, IEEE,
Ang Li, Member, IEEE, Praneeth Vepakomma, Member, IEEE, and Yue Gao, Fellow, IEEE
Abstract—The scalability of large language models (LLMs) in smart healthcare [4], [5], computer vision [6]–[8], intelligent
handlinghigh-complexitymodelsandlarge-scaledatasetshasled transportation [9]–[11] due to their exceptional capabilities in
to tremendous successes in pivotal domains. While there is an
handlinghigh-complexitymodelsandlarge-scaledatasets[1]–
urgentneedtoacquiremoretrainingdataforLLMs,aconcern-
[3], [12]. However, a critical concern accompanying the surge
ing reality is the depletion of high-quality public datasets within
a few years. In view of this, the federated learning (FL) LLM of LLMs has emerged: it is estimated that high-quality public
fine-tuning paradigm recently has been proposed to facilitate datasets will be depleted before 2026 [13]. The increasing
collaborativeLLMfine-tuningondistributedprivatedata,where trend of researchers preferring to train data-hungry LLMs by
multiple data owners collaboratively fine-tune a shared LLM combining existing datasets [14] or utilizing model-generated
without sharing raw data. However, the staggering model size
datasets[15],ratherthancollectingorgeneratingnewdataset,
of LLMs imposes heavy computing and communication burdens
on clients, posing significant barriers to the democratization of also reflects the current scarcity of public available data. This
the FL LLM fine-tuning paradigm. To address this issue, split suggeststhatthedevelopmentofcurrentLLMsmayencounter
learning (SL) has emerged as a promising solution by offloading significant bottlenecks shortly, as the well-established scaling
theprimarytrainingworkloadtoaserverviamodelpartitioning
laws indicate that larger datasets usually lead to better perfor-
while exchanging activation/activation’s gradients with smaller
mance [16].
data sizes rather than the entire LLM. Unfortunately, research
ontheSLLLMfine-tuningparadigmisstillinitsnascentstage. With the rapid proliferation of Internet of Things (IoT)
To fill this gap, in this paper, we propose the first SL LLM devices and advancements in sensor technology, connected
fine-tuningframework,namedSplitLoRA.SplitLoRAisbuilton IoT devices are capable of collecting massive data. However,
thesplitfederatedlearning(SFL)framework,amalgamatingthe these high-quality data distributed across multiple parties
advantagesofparalleltrainingfromFLandmodelsplittingfrom
cannot be shared publicly due to issues such as user privacy
SLandthusgreatlyenhancingthetrainingefficiency.Itisworth
noting that SplitLoRA is the inaugural open-source benchmark concerns(e.g.,medical[17]andfinancial[18]data)orphysical
for SL LLM fine-tuning, providing a foundation for research limitations (e.g., lack of network connectivity). Some leading
efforts dedicated to advancing SL LLM fine-tuning. Extensive AI technology companies can gather large volumes of private
simulations validate that SplitLoRA achieves target accuracy
datatotraindata-hungryLLMs,exemplifiedbyGoogle’sMed-
in significantly less time than state-of-the-art LLM fine-tuning
PaLM [19], a medical LLM capable of providing expert-level
frameworks, demonstrating the superior training performance
of SplitLoRA. The project page is available at https://fdu- medicalguidanceanddiagnosis.Nevertheless,noteveryparty
inc.github.io/splitlora/. possesses adequate private data to independently train a high-
performing and data-hungry LLM. Considering the increasing
Index Terms—Distributed learning, split federated learning,
client-side model aggregation, model splitting, mobile edge com- scarcity of public data and the difficulties in accessing user-
puting. private data, devising collaborative training paradigms for
decentralized private data without data sharing is paramount
I. INTRODUCTION
to driving the advancement of modern LLMs.
In recent years, LLMs [1]–[3] have achieved tremendous The federated learning (FL) LLM training/fine-tuning
successes across a broad spectrum of pivotal domains such as paradigm [20]–[22] has recently been proposed to facilitate
collaborative LLM training on distributed private data, where
Z. Lin, X. Hu, Y. Zhang, Z. Chen, Z. Fang and Y. Gao are multiple data owners collaboratively training a shared LLM
with the School of Computer Science, Fudan University, Shanghai without sharing raw data. In FL LLM paradigm, data owners
200438, China (e-mail: zlin20@fudan.edu.cn; xjhu23@m.fudan.edu.cn;
train local LLMs on their respective local data and then send
yuxinzhang22@m.fudan.edu.cn; zhechen@fudan.edu.cn; zh-
fang19@fudan.edu.cn; gao.yue@fudan.edu.cn). Z. Lin is also with the the LLM parameters rather than raw data to a parameter
Department of Electrical and Electronic Engineering, University

[... Content continues ...]


================================================================================
PAPER: VFLAIR-LLM.pdf
Total Pages: 12
================================================================================

EXTRACTED CONTENT (First 5000 characters):
VFLAIR-LLM: A Comprehensive Framework and Benchmark for
Split Learning of LLMs
ZixuanGu QiufengFan LongSun
SchoolofSoftware,Tsinghua WuxiInnovationCenterofTsinghua WuxiInnovationCenterofTsinghua
University AIR AIR
Beijing,China Wuxi,China Wuxi,China
gu-zx24@mails.tsinghua.edu.cn fan.qiufeng@u.nus.edu cnlonger@gmail.com
YangLiu∗ XiaojunYe
theHongKongPolytechnic SchoolofSoftware,Tsinghua
University University
HongKong,China Beijing,China
yang-veronica.liu@polyu.edu.hk yexj@tsinghua.edu.cn
Abstract LLMs.InProceedingsofthe31stACMSIGKDDConferenceonKnowledge
WiththeadvancementofLargeLanguageModels(LLMs),LLM DiscoveryandDataMiningV.2(KDD’25),August3–7,2025,Toronto,ON,
Canada.ACM,NewYork,NY,USA,12pages.https://doi.org/10.1145/3711
applicationshaveexpandedintoagrowingnumberoffields.How-
896.3737411
ever,userswithdataprivacyconcernsfacelimitationsindirectly
utilizingLLMAPIs,whileprivatedeploymentsincursignificant
computationaldemands.Thiscreatesasubstantialchallengein
1 Introduction
achievingsecureLLMadaptationunderconstrainedlocalresources.
Toaddressthisissue,collaborativelearningmethods,suchasSplit The recent development and success of large language models
Learning (SL), offer a resource-efficient and privacy-preserving (LLMs)havesignificantlyreshapedthelandscapeofartificialintel-
solutionforadaptingLLMstoprivatedomains.Inthisstudy,we ligence,showcasingexceptionalcapabilitiesacrossawiderangeof
introduceVFLAIR-LLM(availableathttps://github.com/FLAIR- tasks.LLMtrainingreliesheavilyonmassive,high-qualitydata,fu-
THU/VFLAIR-LLM),anextensibleandlightweightsplitlearning elinggrowingdemandforsuchresources.Publicdatasources,such
frameworkforLLMs,enablingprivacy-preservingLLMinference asbooks,webcrawls,andopen-accessarticles,havehistorically
andfine-tuninginresource-constrainedenvironments.Ourlibrary servedasthebackboneofLLMtrainingdata.However,research[38]
providestwoLLMpartitionsettings,supportingthreetasktypes indicatesthattheavailabilityofpublichumantextdataisnearing
and18datasets.Inaddition,weprovidestandardmodulesforim- exhaustion.Thisgrowingdatascarcityhasemergedasacritical
plementingandevaluatingattacksanddefenses.Webenchmark5 bottleneckforLLMdevelopment,compellingashifttowardlever-
attacksand9defensesundervariousSplitLearningforLLM(SL- agingprivatedomaindata,whichsubsequentlyraisessignificant
LLM)settings,offeringconcreteinsightsandrecommendationson privacyconcerns.
thechoiceofmodelpartitionconfigurations,defensestrategies,and Sensitivedatawithinprivatedomainscannotbefreelysharedor
relevanthyperparametersforreal-worldapplications. processedbyexternalLLMsystemsduetorisksofdatabreaches,
regulatoryviolations,andpotentialmisuse.Thesechallengesmake
CCSConcepts thedirectintegrationofprivatedataintoLLMtrainingimpractical.
•Securityandprivacy→Distributedsystemssecurity. OnepotentialsolutionisthelocaldeploymentofLLMs.However,
thismethodrequiressubstantiallocalcomputationalresources,pos-
Keywords ingasignificantbarrierforsmallerorganizationsorindividuals
managingsensitiveprivatedata.Toaddressthischallengeofpri-
SplitLearning,LargeLanguageModels,DataPrivacy,Federated
vateadaptationofLLMsunderconstrainedlocalresources,various
Learning
methodshavebeenproposed.Off-sitetuning[42]andknowledge
ACMReferenceFormat: distillation[13]leveragecompactlanguagemodelstoapproximate
ZixuanGu,QiufengFan,LongSun,YangLiu,andXiaojunYe.2025.VFLAIR-
thebehavioroftargetLLMs.However,thesemethodsoftensuf-
LLM:AComprehensiveFrameworkandBenchmarkforSplitLearningof
ferfromnotableperformancedegradationandrequirecomplex
∗Corresponding author, also affiliated with the Shanghai Artificial Intelligence algorithmicimplementations.
Laboratory. AnalternativeapproachisSplitLearning(SL)[10,37],acollab-
orativetrainingparadigmdevelopedbasedonFederatedLearn-
ing(FL)[22,43].Itintroducesacross-siloscenariowhereamodel
ThisworkislicensedunderaCreativeCommonsAttribution4.0InternationalLicense. is partitioned across participants, offering the benefit of minor
KDD’25,Toronto,ON,Canada performancedegradationandasimpleyeteffectivealgorithmic
©2025Copyrightheldbytheowner/author(s).
implementation. However, this solution still faces considerable
ACMISBN979-8-4007-1454-2/2025/08
https://doi.org/10.1145/3711896.3737411 privacy concerns[3], as the server may attempt to infer clients’
5470
KDD’25,August3–7,2025,Toronto,ON,Canada ZixuanGu,QiufengFan,LongSun,YangLiu,andXiaojunYe
local data through various privacy attacks[45]. Various defense forSL-LLM,demonstratingsuperiortrainingperformance.[3]pro-
methods[7,24]havealsobeenproposedtoaddresstheserisks. posedSplitLLM,whichpartitionsthemodelinto3parts:"head",
Inthiswork,wefocusonleveragingSLfortheprivateadaptation "body"and"tail"(termedHBTinthefollowingdiscussion).Itin-
ofLLMs.Aimingtosupportrelevantresearchandapplications,we troduces a novel data reconstruction attack(BiSR, tested in our
design a lightweight and highly extensible Split Learning LLM followingevaluations)toinvertdatainput,highlightingthepo-
framework,namedVFLAIR-LLM. t

[... Content continues ...]

