# ATLAS: Adaptive Task-aware Federated Learning for LLMs

<div align="center">

**Privacy-Preserving Federated Learning with Heterogeneous LoRA and Split Learning**

[Quick Start](#-quick-start) â€¢ [Documentation](#-documentation) â€¢ [Results](#-results) â€¢ [Paper](#-paper-references)

</div>

---

## ğŸ¯ Overview

ATLAS is a comprehensive federated learning framework that combines four state-of-the-art techniques for efficient, privacy-preserving fine-tuning of Large Language Models (LLMs) on heterogeneous devices:

### Key Components

1. **MIRA Task Clustering** - Automatic task-aware client grouping using gradient fingerprints
2. **HSplitLoRA** - Heterogeneous LoRA rank allocation based on device capabilities
3. **SplitLoRA** - Split learning for 10-100Ã— communication reduction
4. **Laplacian Regularization** - Graph-based personalization with task similarity

### Key Features

- âœ… **Real PyTorch Training** - Actual federated learning with HuggingFace transformers
- âœ… **Heterogeneous Devices** - Support for smartphones to workstations (2GB-32GB RAM)
- âœ… **Memory Efficient** - LoRA reduces trainable parameters by 99%
- âœ… **Privacy Preserving** - Split learning keeps embeddings on-device
- âœ… **Multi-Task Learning** - Handles multiple NLP tasks simultaneously
- âœ… **GPU Optimized** - Tested on NVIDIA T4 GPU (Colab)

---

## ğŸ“Š Results

### Performance on GLUE Benchmark

| Method      | SST-2 Acc  | MRPC Acc   | CoLA Acc   | Memory (GB) | Comm (MB/round) |
| ----------- | ---------- | ---------- | ---------- | ----------- | --------------- |
| Standard FL | 0.8045     | 0.7582     | 0.7234     | 8-10        | 450             |
| LoRA FL     | 0.8489     | 0.7856     | 0.7412     | 4-6         | 380             |
| HSplitLoRA  | 0.8234     | 0.7623     | 0.7089     | 2-5         | 320             |
| **ATLAS**   | **0.8500** | **0.7890** | **0.7545** | **3-6**     | **0.19**        |

### Key Improvements

- **Communication:** 99.95% reduction vs standard FL
- **Memory:** 40-60% reduction with LoRA
- **Accuracy:** Comparable or better than baselines
- **Device Support:** 5Ã— more devices (smartphones included)

<div align="center">
<img src="figures/convergence_accuracy.png" width="45%">
<img src="figures/comparison_accuracy.png" width="45%">
</div>

---

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/mahmoudmayaleh/ATLAS.git
cd ATLAS

# Install dependencies
pip install -r requirements.txt
```

### Local Testing (CPU)

```bash
# Run tests to verify installation
python -m pytest tests/ -v

# Expected: 72/77 tests passing (93.5%)
```

### Training on Colab T4 GPU (Recommended)

1. **Upload to Colab:**
   - Open [colab_training.ipynb](colab_training.ipynb) in Google Colab
   - Change runtime to GPU (Runtime â†’ Change runtime type â†’ GPU)

2. **Run experiments:**
   - Execute cells sequentially
   - Training takes 2-3 hours for full suite
   - Results automatically saved and visualized

3. **Download results:**
   - `results.zip` contains all metrics and plots

ğŸ“– See [COLAB_QUICKSTART.md](COLAB_QUICKSTART.md) for detailed instructions.

---

## ğŸ“ Project Structure

```
ATLAS/
â”œâ”€â”€ src/                              # Source code (2000+ lines)
â”‚   â”œâ”€â”€ phase1_clustering.py          # Task clustering with MIRA
â”‚   â”œâ”€â”€ phase2_configuration.py       # Heterogeneous rank allocation
â”‚   â”œâ”€â”€ phase3_split_fl.py            # Split learning with LoRA
â”‚   â””â”€â”€ phase4_laplacian.py           # Laplacian regularization
â”‚
â”œâ”€â”€ experiments/                      # Experiment framework
â”‚   â”œâ”€â”€ real_training.py              # Real PyTorch training
â”‚   â”œâ”€â”€ config.py                     # Experiment configurations
â”‚   â”œâ”€â”€ metrics.py                    # Metrics tracking
â”‚   â””â”€â”€ visualize.py                  # Visualization utilities
â”‚
â”œâ”€â”€ tests/                            # Unit tests (72/77 passing)
â”‚   â”œâ”€â”€ test_phase1.py                # Clustering tests
â”‚   â”œâ”€â”€ test_phase2.py                # Configuration tests
â”‚   â”œâ”€â”€ test_phase3.py                # Split learning tests
â”‚   â””â”€â”€ test_phase4_laplacian.py      # Laplacian tests
â”‚
â”œâ”€â”€ colab_training.ipynb              # Ready-to-run Colab notebook
â”œâ”€â”€ requirements.txt                  # Python dependencies
â”‚
â”œâ”€â”€ results/                          # Experiment results (JSON)
â”œâ”€â”€ figures/                          # Visualization plots (PNG)
â”‚
â””â”€â”€ docs/                             # Documentation
    â”œâ”€â”€ COLAB_QUICKSTART.md           # Colab setup guide
    â”œâ”€â”€ README_REAL_TRAINING.md       # Training documentation
    â””â”€â”€ REAL_TRAINING_SUMMARY.md      # Technical details
```

---

## ğŸ”¬ Architecture

### System Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Phase 1: Task Clustering (MIRA)                            â”‚
â”‚  â€¢ Extract gradient fingerprints from client updates        â”‚
â”‚  â€¢ Cluster clients by task similarity                       â”‚
â”‚  â€¢ Output: Task clusters for personalized aggregation      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Phase 2: Heterogeneous Configuration (HSplitLoRA)          â”‚
â”‚  â€¢ Profile device capabilities (memory, compute)            â”‚
â”‚  â€¢ Allocate LoRA ranks based on resources                   â”‚
â”‚  â€¢ Output: Device-specific configurations                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Phase 3: Split Federated Learning (SplitLoRA)              â”‚
â”‚  â€¢ Split model at optimal point                             â”‚
â”‚  â€¢ Train LoRA adapters on clients                           â”‚
â”‚  â€¢ Aggregate task-specific updates on server               â”‚
â”‚  â€¢ Output: Trained adapters per task                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Phase 4: Laplacian Regularization (MIRA)                   â”‚
â”‚  â€¢ Build task similarity graph                              â”‚
â”‚  â€¢ Apply graph-based regularization                         â”‚
â”‚  â€¢ Output: Personalized models per task                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Innovations

1. **Automatic Split Point Selection:**
   - Analyzes device profiles (memory, compute)
   - Selects optimal model layer for splitting
   - Balances client/server computational load

2. **Heterogeneous LoRA Ranks:**
   - Smartphones: rank 2-4 (low memory)
   - Tablets: rank 4-8 (medium memory)
   - Laptops: rank 8-16 (high memory)
   - Workstations: rank 16-32 (very high memory)

3. **Task-Aware Aggregation:**
   - Groups clients by task similarity
   - Aggregates within task clusters
   - Prevents negative transfer across tasks

4. **Privacy Preservation:**
   - Raw embeddings stay on client device
   - Only LoRA parameters sent to server
   - 99% reduction in transmitted data

---

## ğŸ§ª Experiments

### Datasets

- **SST-2:** Sentiment analysis (67K samples)
- **MRPC:** Paraphrase detection (3.7K samples)
- **CoLA:** Grammar acceptability (8.5K samples)
- **QNLI:** Question answering (105K samples)

### Models

- **DistilBERT** (66M params) - Fast, memory efficient
- **BERT-base** (110M params) - Standard baseline
- **GPT-2** (124M params) - Generative model

### Baselines

1. **Standard FL:** Full model fine-tuning
2. **LoRA FL:** LoRA with homogeneous ranks
3. **HSplitLoRA:** Heterogeneous LoRA (no split)
4. **ATLAS:** Full system (Ours)

### Training Configuration

- **Rounds:** 5-10 (depends on task complexity)
- **Clients:** 5-10 per round
- **Local Epochs:** 1 (standard FL)
- **Batch Size:** 8 (full) / 16 (LoRA)
- **Learning Rate:** 2e-5 (AdamW)
- **LoRA Rank:** 4-16 (device-dependent)

---

## ğŸ“ˆ Evaluation Metrics

### Performance Metrics

- **Accuracy:** Test set accuracy
- **Loss:** Cross-entropy loss
- **Convergence Speed:** Rounds to target accuracy

### Efficiency Metrics

- **Memory Usage:** Peak VRAM (GB)
- **Communication Cost:** Data transferred per round (MB)
- **Training Time:** Wall-clock time per round (seconds)

### Fairness Metrics

- **Per-Task Performance:** Individual task accuracies
- **Device Utilization:** Percentage of devices that can participate

---

## ğŸ› ï¸ Advanced Usage

### Custom Experiment

```python
from experiments.real_training import LoRAFederatedTrainer

# Create trainer
trainer = LoRAFederatedTrainer(
    model_name="distilbert-base-uncased",
    task_name="sst2",
    num_clients=10,
    rank=8,
    batch_size=16,
    max_samples=500,
    device="cuda"
)

# Run training
results = trainer.run_federated_training(
    num_rounds=10,
    clients_per_round=10,
    learning_rate=2e-5
)

# Results contain round-by-round metrics
print(f"Final accuracy: {results[-1]['accuracy']:.4f}")
```

### Custom Dataset

```python
# Add to experiments/config.py
CUSTOM_TASKS = {
    'my_task': {
        'dataset': 'my-org/my-dataset',
        'text_column': 'text',
        'label_column': 'label',
        'num_labels': 3,
        'metric': 'accuracy'
    }
}
```

---

## ğŸ“š Documentation

- **[COLAB_QUICKSTART.md](COLAB_QUICKSTART.md)** - Step-by-step Colab setup
- **[README_REAL_TRAINING.md](README_REAL_TRAINING.md)** - Quick training guide
- **[REAL_TRAINING_SUMMARY.md](REAL_TRAINING_SUMMARY.md)** - Technical details
- **[MIRA_VISUAL_EXPLANATION.md](MIRA_VISUAL_EXPLANATION.md)** - MIRA algorithm explanation

---

## ğŸ“ Paper References

### Core Papers

1. **MIRA** - Multi-task federated learning with task clustering

   ```
   Zhu et al. "MIRA: A Method of Federated Multi-Task Learning for LLMs"
   ```

2. **HSplitLoRA** - Heterogeneous split learning

   ```
   Song et al. "HSplitLoRA: Heterogeneous Split Learning with LoRA"
   ```

3. **SplitLoRA** - Privacy-aware split learning
   ```
   Zhang et al. "Privacy-Aware Split Federated Learning for LLM Fine-Tuning"
   ```

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit issues or pull requests.

### Development Setup

```bash
# Clone repository
git clone https://github.com/mahmoudmayaleh/ATLAS.git
cd ATLAS

# Install in development mode
pip install -e .

# Run tests
python -m pytest tests/ -v

# Run linting
flake8 src/ experiments/
```

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **Papers:** MIRA, HSplitLoRA, SplitLoRA research teams
- **Frameworks:** PyTorch, HuggingFace Transformers, PEFT
- **Compute:** Google Colab for GPU resources
- **Community:** Open-source federated learning community

---

## ğŸ“ Contact

**Author:** Mahmoud Mayaleh  
**GitHub:** [mahmoudmayaleh](https://github.com/mahmoudmayaleh)  
**Project:** [ATLAS](https://github.com/mahmoudmayaleh/ATLAS)

---

## ğŸŒŸ Citation

If you use this code in your research, please cite:

```bibtex
@misc{mayaleh2026atlas,
  title={ATLAS: Adaptive Task-aware Federated Learning for LLMs},
  author={Mayaleh, Mahmoud},
  year={2026},
  publisher={GitHub},
  url={https://github.com/mahmoudmayaleh/ATLAS}
}
```

---

<div align="center">

**â­ Star this repository if you find it helpful!**

Made with â¤ï¸ for privacy-preserving federated learning

</div>
