{
  "ATLAS_Base_Specification.pdf": {
    "pages": 1,
    "content": "Conservatoire national des arts et métiers Mahmoud Mayaleh\nUS PROJ 2025/2026 Project # 13\nLyes Khoukhi & Zakria Abouelhouda\nImplementation and evaluation of efficient methods for fine-tuning large-scale language\nmodels (LLM) in a federated learning framework\nBase Specification\nThe widespread adoption of large language models has transformed natural language processing, yet their\ndeployment on edge devices faces two critical challenges. Models with billions of parameters exceed the\nmemory and computational capacity of individual devices, requiring collaborative training across multiple\nnodes. Additionally, clients operate on heterogeneous task-specific data such as medical records, financial\ntransactions, or conversational logs, making naive parameter sharing inefficient or harmful. Existing\nsolutions address only one challenge: federated multi-task learning frameworks like MIRA provide task\nawareness through centralized coordination but cannot split models across devices, while peer-to-peer\nsplit learning approaches like TITANIC enable model partitioning but remain task-blind under data\nheterogeneity.\nThe primary objective of this project is to design, implement, and evaluate ATLAS, a self-organizing\ndecentralized multi-task learning framework for large language model fine-tuning on heterogeneous edge\ndevices. ATLAS combines gradient-based semantic client clustering with peer-to-peer split learning and\ncluster-wise adapter aggregation to enable collaborative fine-tuning without centralizing raw data or\nrequiring any device to host the entire model. The framework automatically discovers task-aware\nsemantic neighborhoods of clients with similar workloads and exploits these neighborhoods for efficient\nrouting and aggregation.\nThe first phase involves comprehensive analysis of federated multi-task learning, split learning, and\nparameter-efficient adaptation techniques. This includes studying how MIRA leverages centralized task\ngraphs while identifying its bottlenecks, examining TITANIC's peer-to-peer architecture and its\nlimitations under task heterogeneity, and reviewing adapter-based methods like LoRA for decentralized\naggregation. The outcome is an architectural specification defining task embeddings, clustering\nprocedures, resource-aware pairing constraints, and hierarchical aggregation mechanisms.\nThe second phase focuses on implementing a simulation environment instantiating the ATLAS lifecycle.\nClients compute gradient-based task embeddings from local data and send compact signatures to a server\nthat performs clustering to identify semantic neighborhoods. The server initializes shared LoRA adapters\nand returns peer partnership information. During training, clients execute split learning by partitioning\nmodel layers according to memory constraints and exchange intermediate activations directly. Robustness\nmechanisms including intra-cluster re-pairing and inter-cluster fallback handle stragglers and client\ndropouts.\nThe final phase conducts systematic experimental evaluation measuring task-specific accuracy,\ncommunication volume, convergence speed, computational overhead, and semantic clustering quality.\nATLAS will be compared against centralized training, federated multi-task baselines, and task-blind split\nlearning to characterize the benefits of combining semantic routing with peer-to-peer model partitioning\nand derive actionable deployment guidelines.\n"
  },
  "ATLAS_V1.pdf": {
    "pages": 22,
    "content": "ATLAS\nAdaptive Task-aware Layered Aggregation for Split Learning\nYour Name\nDecember 23, 2025\nWhy ATLAS? - Challenge 1: Model Size\nLLMs are too large for edge devices\nModern models: 7B to 70B parameters\nFar exceed memory constraints of:\nMobile phones\nIoT devices\nEdge servers\nProblem: Inference and training are infeasible on weak devices\nYourName ATLAS December23,2025 2/22\nWhy ATLAS? - Challenges & Device Heterogeneity\nChallenge 2: Heterogeneity\nGPU\nDifferent device budgets (1GB 24GB Strong:\nfullyused\nto 24GB GPU) Client A\nDifferent task types (NLP, GPU\nQ&A, summarization) 8GB Medium:\nbalanced\nClient B\nDifferent network speeds (5G,\n4G, WiFi)\nGPU\n1GB Weak:\nClientC limited\nChallenge 3: Efficiency\nOne-size-fits-all approach\nWastes resources on weak\ndevices\nUnderutilizes strong devices\nYourName ATLAS December23,2025 3/22\nStanding on Existing Work: SplitLoRA\nSplitLoRA\nSplit model between client and server\nTrainable LoRA adapters for efficiency\nHomogeneous setup (all clients identical)\nAdvantage: Efficient communication\nLimitation: No heterogeneity support\nYourName ATLAS December23,2025 4/22\nStanding on Existing Work: HSplitLoRA\nHSplitLoRA\nHeterogeneous LoRA ranks per client\nAdapted to device budgets\nDifferent configurations per device\nAdvantage: Efficient + heterogeneous\nLimitation: No task awareness (ignores task similarity)\nYourName ATLAS December23,2025 5/22\nStanding on Existing Work: MIRA\nMIRA\nTask clustering for small models\nClients in same cluster share configurations\nKnowledge sharing between similar tasks\nAdvantage: Efficient + task-aware\nLimitation: Not designed for split learning with LLMs\nYourName ATLAS December23,2025 6/22\nComparison: ATLAS vs. Existing Methods\nMethod Heterogeneous Task-Aware Efficient\nSplitLoRA No No Yes\nHSplitLoRA Yes No Yes\nMIRA Yes Yes No\nATLAS Yes Yes Yes\nATLAS = Combining all three ideas for LLMs\nYourName ATLAS December23,2025 7/22\nATLAS: System Overview\nATLAS operates in four phases:\n1 Phase 1: Discover similar tasks through clustering\n2 Phase 2: Configure device-specific LoRA ranks\n3 Phase 3: Train using split learning\n4 Phase 4: Aggregate updates per cluster\nResult: Federated LLM training on heterogeneous devices with task\nawareness\nYourName ATLAS December23,2025 8/22\nPhase 1: Discover Similar Tasks - Goal\nGoal: Group clients with similar tasks to enable knowledge sharing\nProcess:\n1 Each client computes gradients on its own data (small batch)\n2 Compress gradients into 64-dimensional “task fingerprint”\n3 Server collects all fingerprints\n4 k-Means clustering groups similar tasks\nPrivacy: Only low-dim vectors sent; no raw data or full gradients\nYourName ATLAS December23,2025 9/22\nClients 1,2; [ellipse, draw, fill=green!20, minimum width=1.5cm,\nright=2.5cm of f3] (cl2) Cluster 2\nClient 3;\n[-¿, thick] (c1) – (f1); [-¿, thick] (c2) – (f2); [-¿, thick] (c3) – (f3); [-¿,\nthick] (f1) – (cl1); [-¿, thick] (f2) – (cl1); [-¿, thick] (f3) – (cl2);\nYourName ATLAS December23,2025 10/22\nPhase 2: Assign Heterogeneous Configurations\nGoal: Choose LoRA rank and split point for each client\nProcess:\n1 Determine weight importance per task cluster\n2 Assign higher ranks to important weights\n3 Respect each client’s memory budget\n4 Similar rank patterns within clusters, different absolute values\nExample:\nClient A (24GB): ranks [8, 6, 4]\nClient B (8GB): ranks [6, 4, 2]\nClient C (1GB): ranks [4, 2, 1]\nYourName ATLAS December23,2025 11/22\nPhase 3: Train with Split Learning\nGoal: Train LoRA adapters using client-server split architecture\nArchitecture:\nClient: Input processing + early layers + LoRA adapters\nServer: Task layers + loss head + LoRA adapters\nPer round:\n1 Client sends intermediate activations h to server\n2 Server computes loss and gradients ∇h\n3 Server sends gradients back to client\n4 Both update LoRA adapters using local SGD\nKey: No full model or gradients transmitted; minimal communication\nYourName ATLAS December23,2025 12/22\nPhase 4: Aggregate Updates Per Cluster\nGoal: Merge LoRA adapters from same cluster for global model\nKey Innovation: Noise-Free Aggregation\nEven with different ranks, we can merge perfectly:\nClient 1: rank 4, Client 2: rank 6, Client 3: rank 4\nConcatenate low-rank factors ⇒ rank 14\nMerge using aggregation formula\nResult: quality of averaging, no information loss\nProcess:\n1 Stack all client adapters\n2 Apply weighted average in merged space\n3 Broadcast updated adapter back\n4 Clients decompose for next round\nYourName ATLAS December23,2025 13/22\nATLAS Advantages: Efficiency (30-40% Memory Saved)\nWeak devices get small LoRA ranks ⇒ fit within budget\nStrong devices use larger ranks ⇒ utilize resources\nSplit learning minimizes communication overhead\nHeterogeneous config prevents over-provisioning\nResult: Significant memory savings while maintaining performance\nYourName ATLAS December23,2025 14/22\nATLAS Advantages: Accuracy (20-30% Gain)\nTask clustering enables knowledge transfer\nClients specialize within clusters\nShared cluster-level model provides regularization\nLoRA ranks ensure high-capacity learning\nResult: Improved convergence and final accuracy\nYourName ATLAS December23,2025 15/22\nATLAS Advantages: Privacy\nOnly 64-D fingerprints sent (not raw data)\nNo full gradients or model updates transmitted\nOptional: add differential privacy (DP) noise\nEvaluated using VFLAIR-LLM framework\nResult: Privacy-aware federated learning\nYourName ATLAS December23,2025 16/22\n12-Week Implementation Plan\n1 Weeks 1–2: PyTorch + Transformers setup + basic split learning\n2 Weeks 3–4: Add LoRA from PEFT, test on GPT-2\n3 Weeks 5–6: Phase 1 (task clustering with gradient PCA)\n4 Weeks 7–8: Phase 2 (weight importance + rank selection)\n5 Weeks 9–10: Phase 3 training loop + Phase 4 aggregation\n6 Weeks 11–12: Evaluation + privacy check with VFLAIR\nTech Stack: PyTorch, HF Transformers, PEFT (LoRA), scikit-learn\n(k-means, PCA)\nYourName ATLAS December23,2025 17/22\nEvaluation Plan\nMetrics:\nAccuracy: Task-specific metrics, convergence speed\nEfficiency: GPU memory, communication cost, training time\nPrivacy: Attack success rate (VFLAIR), privacy-utility trade-off\nBaselines:\nSplitLoRA (homogeneous)\nHSplitLoRA (heterogeneous, no clustering)\nCentralized fine-tuning (upper bound)\nExpected Results:\nAccuracy: +15–25% vs HSplitLoRA\nMemory: 30–40% reduction\nPrivacy: DCS ≥ 0.7\nYourName ATLAS December23,2025 18/22\nOpen Questions for Supervisors\nSystem Design:\nFixed or adaptive split point?\nHow many clusters K? (e.g., 3, 5, 10)\nWithin-cluster regularizer?\nExperiments:\nWhich datasets? (FLAN, multi-task NLP)\nHow many clients? (10, 50, 100, 1000)\nWhich LLM? (GPT-2 first, then LLaMA-7B)\nResources:\nVFLAIR-LLM access or implement privacy attacks?\nGPU hour budget?\nTimeline flexibility?\nYourName ATLAS December23,2025 19/22\nSummary: ATLAS\nATLAS = Smart heterogeneity + Task awareness + Efficient\naggregation\nCore operations:\n1 Discover similar tasks through clustering\n2 Configure budget-aware LoRA ranks\n3 Train via split learning\n4 Aggregate without information loss\nNovelty: Combines 3 existing ideas (MIRA + HSplitLoRA + SplitLoRA)\nfor LLMs\nFeasibility: 12-week timeline, all open-source, no novel breakthroughs\nYourName ATLAS December23,2025 20/22\nTechnology Stack (Open-Source Guarantee)\nAll open-source:\nPyTorch (BSD)\nHuggingFace Transformers (Apache 2.0)\nPEFT LoRA (Apache 2.0)\nscikit-learn (BSD)\nVFLAIR-LLM (MIT)\nNOT used:\nTitanic (proprietary)\nAny closed-source framework\nYourName ATLAS December23,2025 21/22\nWhat We Need from Supervisors\n1 ✓Research Direction Approval\n2 ✓GPU Compute Budget (e.g., 1000 GPU-hours)\n3 ✓Dataset/Task Specification (which NLP tasks)\n4 ✓Timeline Confirmation (12 weeks feasible?)\nATLAS is feasible, novel, and ready to build!\nYourName ATLAS December23,2025 22/22\n"
  },
  "HSplitLoRA.pdf": {
    "pages": 16,
    "content": "1\nHSplitLoRA: A Heterogeneous Split\nParameter-Efficient Fine-Tuning Framework for\nLarge Language Models\nZheng Lin, Yuxin Zhang, Zhe Chen, Member, IEEE, Zihan Fang, Xianhao Chen, Member, IEEE, Praneeth\nVepakomma, Member, IEEE, Wei Ni, Fellow, IEEE, Jun Luo, Fellow, IEEE, and Yue Gao, Fellow, IEEE\nAbstract—Recently, large language models (LLMs) have in both industry and academia. These models excel in ex-\nachieved remarkable breakthroughs, revolutionizing the natural tracting intricate patterns from vast datasets and adapting\nlanguageprocessingdomainandbeyond.Duetoimmenseparam-\nto diverse downstream tasks, ranging from natural language\neter sizes, fine-tuning these models with private data for diverse\nunderstanding and generation to specialized domains such\ndownstream tasks has become mainstream. Though federated\nlearning (FL) offers a promising solution for fine-tuning LLMs as smart healthcare [6], [7], finance [8], and intelligent\nwithoutsharingrawdata,substantialcomputingcostshinderits transportation [9], [10]. However, due to immense parameter\ndemocratization.Moreover,inreal-worldscenarios,privateclient sizes, training LLMs from scratch is typically infeasible [11],\ndevicesoftenpossessheterogeneouscomputingresources,further\nrendering fine-tuning with private data for specific down-\ncomplicating LLM fine-tuning. To combat these challenges, we\nstreamtasksbecomemainstream.Theworkflowoffine-tuning\npropose HSplitLoRA, a heterogeneous parameter-efficient fine-\ntuning (PEFT) framework built on split learning (SL) and low- LLMs typically follows a three-stage process: i) pre-training\nrank adaptation (LoRA) fine-tuning, for efficiently fine-tuning LLMsfromscratchonextensivetextcorpora(e.g.,Wikipedia)\nLLMsonheterogeneousclientdevices.HSplitLoRAfirstidentifies using substantial computing resources (e.g., training GPT3-\nimportantweightsbasedontheircontributionstoLLMtraining.\n1.3B model requires 64 Tesla V100 GPUs running for one\nItthendynamicallyconfiguresthedecompositionranksofLoRA\nweek [12]); ii) fine-tuning pre-trained LLMs to adapt to\nadapters for selected weights and determines the model split\npoint according to varying computing budgets of client devices. specificdownstreamtasks;iii)deployingthefine-tunedLLMs\nFinally,anoise-freeadapteraggregationmechanismisdevisedto on client devices to perform inference.\nsupport heterogeneous adapter aggregation without introducing Unfortunately, privacy concerns often hinder the collabora-\nnoise. Extensive experiments demonstrate that HSplitLoRA out-\ntive fine-tuning of LLMs across multiple parties, even when\nperforms state-of-the-art benchmarks in training accuracy and\ntheysharethesamedownstreamtaskandcouldmutuallybene-\nconvergence speed.\nfit.Forexample,severalfreelancefinancialanalystsmaywish\nIndex Terms—Distributed learning, split learning, large lan-\ntocollaborativelyfine-tuneanLLMforaspecialfinancialrisk\nguage model, parameter-efficient fine-tuning.\npredictiontaskviatheirpersonaldevices,buttheyarereluctant\nI. INTRODUCTION to share customer data [8]. Federated learning (FL) [13],\n[14] offers a promising solution to this privacy issue [15],\nRecently, large language models (LLMs) have achieved\n[16] by enabling participants to fine-tune LLMs using local\ntremendoussuccessacrossabroadspectrumofpivotalsectors\ndata without sharing the raw data [17], [18]. However, fine-\ndue to their exceptional ability in handling high-complexity\ntuning LLMs via FL is non-trivial, particularly for resource-\nand large-scale datasets [1]–[5]. Notable examples include\nconstrained client devices. In typical configurations, client\nOpenAI’s GPT series [1], Google’s Gemini [2], and Meta’s\ndevices are equipped with low-cost commercial GPUs, such\nLLaMA [3], all of which have gained significant attention\nas the NVIDIA GeForce RTX 3050 in a laptop. While these\nGPUs are sufficient for conventional deep learning tasks such\nZ. Lin, Y. Zhang, Z. Chen, Z. Fang, and Y. Gao are with the Institute\nas object detection [19] and image classification [20], they\nof Space Internet, Fudan University, Shanghai 200438, China, and the\nSchoolofComputerScience,FudanUniversity,Shanghai200438,China(e- can be overwhelmed by the immense computing workload\nmail:zlin20@fudan.edu.cn;zhfang19@fudan.edu.cn;zhechen@fudan.edu.cn; of full parameter fine-tuning of LLMs within standard FL\nyxzhang24@m.fudan.edu.cn;gao.yue@fudan.edu.cn).Z.Linisalsowiththe\nframeworks. To combat this issue, some recent studies [21],\nDepartment of Electrical and Electronic Engineering, University of Hong\nKong,PokFuLam,HongKong,China. [22] focus on integrating FL with parameter-efficient fine-\nX. Chen is with the Department of Electrical and Electronic Engineer- tuning (PEFT) methods, such as Adapter [23] and low-rank\ning, University of Hong Kong, Pok Fu Lam, Hong Kong, China (e-mail:\nadaptation (LoRA) [11]. Despite these efforts, fine-tuning\nxchen@eee.hku.hk).\nP.VepakommaiswithMohamedbinZayedUniversityofArtificialIntelli- LLMs via FL on client devices remains prohibitively time-\ngence,AbuDhabi,UnitedArabEmirates,andtheMassachusettsInstituteof and resource-consuming. For instance, fine-tuning LLaMA 3-\nTechnology,Cambridge,MA02139USA(e-mail:vepakom@mit.edu).\n8B [3] requires over 20GB of GPU memory, which exceeds\nW. Ni is with Data61, CSIRO, Marsfield, NSW 2122, Australia, and the\nSchool of Computing Science and Engineering, and the University of New the capabilities of most client devices.\nSouthWales,Kennington,NSW2052,Australia(e-mail:wei.ni@ieee.org). Split learning (SL) [24] has emerged as a compelling\nJ.LuoiswiththeSchoolofComputerEngineering,NanyangTechnological\nalternative capable of overcoming the weaknesses of FL. SL\nUniversity,Singapore(e-mail:junluo@ntu.edu.sg).\n(Correspondingauthor:YueGao) offloads the primary training workload to a central server\n5202\nyaM\n5\n]GL.sc[\n1v59720.5052:viXra\n2\nmethod,namedHSplitLoRA.First,inordertoefficientlyfine-\nClient-side LLM\ntune the trainable weights under computing resource con-\nFine-tuned parameters\naggregation straints, we design an important weight identification scheme\nFine-tuned Activations\nparameters transmission Server-side LLM to identify important weights based on their contributions to\nuploading Client device 1\nLLM training. Second, to accommodate the heterogeneous\ncomputingcapabilitiesofedgeservers,weproposeadynamic\nrank and model splitting configuration to dynamically adjust\nthedecompositionranksofLoRAadaptersandthemodelsplit\nClient device 2\nFed server ... Central server point to align with client devices’ computing budgets. Finally,\nFine-tuned Gradients to seamlessly aggregate heterogeneous LoRA adapters, we\np d a o r w a l m oa e d te in r g s transmission develop a noise-free adapter aggregation that meticulously\nClient device N concatenates low-rank decomposition matrices to support het-\nActivations/gradients Fine-tuned parameters Fine-tuned Freezed erogeneousadapteraggregationwithoutintroducingnoise.The\ntransmission uploading/dowloading parameters parameters\nkey contributions of this paper are summarized as follows:\nFig. 1: A scenario of LLM SL via PEFT with client devices.\n• To the best of our knowledge, HSplitLoRA is the first\nheterogeneousLLMfine-tuningframeworkthatintegrates\nSL and PEFT, enabling distributed collaborative learning\nvia layer-wise model partitioning, alleviating the computing\nacross heterogeneous client devices.\nburden on resource-constrained client devices [25]. Fig. 1\nillustrates how PEFT integrates with the state-of-the-art SL • We devise an important weight identification scheme to\nquantify the contribution of trainable weights to training\nimplementation, split federated learning (SFL) [26], for LLM\nperformance to prioritize important weights for fine-\nfine-tuning.Thepre-trainedLLMissplitintotwosub-models:\ntuning.\nclient-side LLM deployed on the client devices and server-\nside LLM deployed on a central server. Each client device • We propose an adaptive rank and model splitting con-\nfiguration to adjust the decomposition ranks of LoRA\nexecutes forward propagation on the client-side LLM and\nadapters and the model split point to accommodate het-\ntransmits activations to the central server. Then, the central\nerogeneous computing budgets of client devices.\nserverleveragesthereceivedactivationstofine-tunetheserver-\nside LLMs and send gradients to client devices for client- • We develop a noise-free adapter aggregation that metic-\nulously concatenates low-rank decomposition matrices\nside back-propagation. After the training is completed, the\nto support heterogeneous adapter aggregation without\nfedserveraggregatestheclient-sidefine-tunedparametersand\nintroducing additional noise.\nredistributes them to client devices. This process repeats until\nthe LLM reaches a pre-defined accuracy threshold. • We empirically evaluate HSplitLoRA with extensive\nexperiments. The results demonstrate that HSplitLoRA\nWhile integrating SL and PEFT techniques holds signif-\noutperforms state-of-the-art benchmarks, significantly in\nicant promise, its implementation encounters several critical\nchallenges. First, the limited computing resources1 of client training accuracy and convergence speed.\ndevicesrenderfine-tuningalltrainableparametersimpractical, Therestofthispaperisorganizedasfollows.SectionIIin-\nnecessitating selectively fine-tuning a portion of trainable troducesthebackgroundandmotivation.SectionIIIelaborates\nweights. However, this paradigm remains unexplored in the on the system design of HSplitLoRA. Section IV presents the\nSL paradigm. Second, the inherent heterogeneity of comput- system implementation, followed by the performance evalu-\ning resources across client devices leads to a severe device ation in Section V. Related works and technical limitations\nunavailability problem2 [30]–[32], ultimately degrading the are discussed in Section VI. Finally, conclusions and future\ntrainingperformance.Lastbutnotleast,thevaryingcomputing remarks are presented in Section VII.\nbudgets across client devices result in structural discrepancies\nII. BACKGROUNDANDMOTIVATION\nin fine-tuning adapters, making it infeasible to aggregate\nIn this section, we begin by introducing the background\nthese adapters across diverse client devices. Empirical studies\nof PEFT for LLM SL. Then, we conduct extensive pilot\nhave been conducted to validate these challenges, as will be\nmeasurement studies to investigate the design challenges of\ndiscussed in Section II.\nSL on LLM PEFT, serving as the foundation for the design\nTo tackle these challenges, we propose a heterogeneous\nmotivation behind HSplitLoRA.\nPEFTframeworkbuiltonsplitlearningandLoRAfine-tuning\nA. Injecting PEFT into Split Learning for LLMs\n1Inthispaper,weuseGPUmemoryfootprintasaproxymetrictoquantify\nThe prevalent LLMs typically consist of billions or even\ncomputing resource availability [27]–[29]. This is because GPU memory\nfootprint directly correlates with a model’s computing demands, especially hundreds of billions of parameters. The immense scale of\nforLLMfine-tuning,wherememory-intensiveoperationssuchasmulti-head LLMs demands substantial computing resources, rendering\nself-attentionandlarge-scalematrixmultiplicationsdominate.GPUmemory\nfull-parameterfine-tuning(FT)[34](i.e.,initializingthemodel\nfootprint provides an effective proxy for evaluating the feasibility of model\nfine-tuningonresource-constrainedclientdevices. with pre-trained weights and updating all parameters to adapt\n2Thedeviceunavailabilityproblemoccurswhenparticipatingdevicesfail to various downstream tasks) of LLMs infeasible, especially\nto perform local model training or data transmission due to disconnections,\non resource-constrained client devices. While the SL can\nnetworkinstability,orinsufficientcomputingresources,thushinderingglobal\nmodelupdatesanddegradingtrainingperformance. offload primary computing burdens to central servers via\n3\nA\nA\nF\nd\nd\ne\nd\nd\ne d\n&\n&\nA\nF\nd\nL\nL\no\na\nr\na\na\nw\np\ny\ny\nt\ne\ne\na\ne\nr\nr\nr\nr\nd\nN\nN\n(\no\no\nW\nr\nr\nm\nm\no)\nL\nNon\nW\nlin\nu\ne\np\nar\ngnitupm\noC dezilam roN\n)01gol(\ndaehrevO\n1\n1 1\n1\n1\n0\n0 0\n0\n0\n-\n- -\n-\n4\n3 2\n1\n0\nr=2 r=4 r=8 r=16 FT\nnoitacinum\nm oC dezilam\nroN\n)01gol(\ndaehrevO\n1\n1 1\n1\n1\n0\n0 0\n0\n0\n-\n- -\n-\n4\n3 2\n1\n0 S\nL\nm\no\nr\nR\na\n=\ns\nA\n2\nh e\na\nd\nd a\nd\np\na\nr\nt\nt\ne\n=\na\nr\n4 r=8 r=16 FT\nAdapter\nWdown\n(a)Computingoverhead (b)Communicationoverhead\nFig. 3: The normalized computing and communication over-\nAttention\nAdapter heads of LoRA and FT on LLaMA-2-7B, where r is the\nQ K V decomposition rank of LoRA adapter.\nWq LoRA Wk LoRA Wv\nB A global model. We select the E2E dataset [38] as the training\ndataset. Unless otherwise specified, this experimental setup\nHidden States\nMulti-Head LoRA adapter remains the same throughout the whole Section II. Fig. 3a\nand 3b present the normalized computing and communication\nInput data overhead3 for LoRA and FT. The results reveal that the\ncomputing and communication overhead of FT is more than\nFig. 2: An illustration of Adapter [23] and LoRA [11] fine-\n500 and 100 times that of LoRA. Apparently, instead of\ntuning methods, where W , W , W , and W are trainable\nq k v o the FT, PEFT can significantly mitigate the computing and\nweights [33].\ncommunication overhead of LLM SL. However, we still meet\nthreekeychallengesinintegratingPEFTintoLLMSL,which\nwill be discussed in the following sections.\nmodel partitioning [33], fine-tuning client-side LLMs still\nleads to intolerable latency. For instance, the client-side LLM B. Limited Computing Resources\ncomposedofthefirst10TransformerblocksfromtheLLama- While LoRA significantly reduces the computing overhead\n2-7B model [3] contains 8.75 billion parameters—60 times for fine-tuning LLMs (see Section II-A), configuring LoRA\nmorethantheparametercountoftraditionalmodelslikeVGG- adapterstoalltrainableweightsonresource-constrainedclient\n16 [35], which has only 139 million parameters. devices is still impractical as LLM scale up [11], [39], [40].\nTo address these challenges, PEFT offers a promising so- In SL, this limitation also holds for the central server, which\nlution by fine-tuning only a small portion of LLM parame- needs to handle heavy workloads offloaded from multiple\nters, significantly reducing the computing cost. The current client devices. Despite its more powerful computing capabil-\ntwo widely adopted PEFT methods are Adapter [23] and ity, the central server still struggles to accommodate LoRA\nLoRA [11]. As shown in Fig. 2, Adapter works by inserting a configurations for all trainable weights as the number of\nfew additional trainable layers into the model backbone [23] served client devices increases [20], [41]. Therefore, it is\nand only fine-tuning those layers in a pre-trained model. In imperative to selectively fine-tune trainable weights under\ncontrast, LoRA concatenates two trainable low-rank decom- resource constraints.\nposition matrices in parallel to weights [11] but freezes all To understand how the selection of trainable weights im-\nweights of a pre-trained model. For LoRA fine-tuning, the pactsthetrainingperformanceof"
  },
  "MIRA_A_Method_of_Federated_Multi-Task_Learning_for_Large_Language_Models.pdf": {
    "pages": 5,
    "content": "IEEENETWORKINGLETTERS,VOL.7,NO.3,SEPTEMBER2025 171\nMIRA: A Method of Federated Multi-Task Learning for Large Language Models\nAhmed Elbakary , Graduate Student Member, IEEE, Chaouki Ben Issaid , Member, IEEE, Tamer ElBatt,\nKarim Seddik , Senior Member, IEEE, and Mehdi Bennis , Fellow, IEEE\nAbstract—Inthisletter,weintroduceamethodforfine-tuning jointly to solve a specific problem by leveraging local data\nLargeLanguageModels(LLMs),inspiredbyMulti-Tasklearning for local training and sharing only the model’s parameters,\nin a federated manner. Our approach leverages the structure\ninsteadofthelocaldata.TraditionalFLalgorithmsaimtotrain\nof each client’s model and enables a learning scheme that\na global model that can be deployed across all participating\nconsiders other clients’ tasks and data distribution. To mitigate\nclients, regardless of the underlying distribution of their data.\nthe extensive computational and communication overhead often\nassociatedwithLLMs,weutilizeaparameter-efficientfine-tuning Multiple frameworks already exist to leverage FL for fine-\nmethod,specificallyLow-RankAdaptation(LoRA),toreducethe tuningLLMs.Forexample,Zhangetal.[4]proposedtheusage\nnumber of trainable parameters. Experimental results, with dif- of different Parameter-efficient fine-tuning (PEFT) methods\nferent datasets and models, demonstrate the proposed method’s\nlike prompt tuning to make the fine-tuning process of LLMs\neffectivenesscomparedtoexistingframeworksforfederatedfine-\ncommunicationefficient.Theideaofprompttuningistoadda\ntuning of LLMs in terms of global and local performances. The\nproposed scheme outperforms existing baselines by achieving setofvirtualtokens,oftencalledasoftprompt,totheoriginal\nlower local loss for each client, while maintaining comparable prompt,i.e.,thehardprompt.Then,ittriestooptimizethesoft\nglobal performance. prompttogeneratethedesiredoutput.In[5],theauthorsmade\nIndex Terms—Large language models, federated learning, useofthefactthatmostFLsettingsoperatewithedgedevices\nmulti-task learning, deep neural networks. designed for inference not training. This characteristic leads\nto solving the problem using a zeroth-order method without\nthe need for backpropagation (BP).\nI. INTRODUCTION\nThe main problem with the existing federated fine-tuning\nPRE-TRAINED Large Language Models (LLMs) are approaches for LLMs is the fact that they all learn a single\nproventobefew-shotlearners[1],whichmeansthatthey global model based on model averaging, which might not\ncan perform a diverse set of tasks without retraining on those be the optimal solution for highly heterogeneous settings.\nspecifictasks.Thispropertyisattributedmainlytotwofactors: Moreover, a single global model might not be the optimal\nthe wide range of their training data and the scale of the solution when the model is expected to perform well across\narchitecture.Whilein-contextlearning[2]mightbeenoughto different tasks where the data distribution differs significantly.\nadapt the LLM to the task at hand, this might not be possible For example, different medical entities around the world\nwithout further model fine-tuning. The problem arises when might train a chatbot system to interact with their patients.\nadapting these models to a domain-specific area where the A single global model may fail to capture the nuanced med-\ndata comes from a different distribution than the training data ical terminologies, treatment protocols, or diagnostic criteria\nitself. This requires further fine-tuning of the models so they uniquetoeachregionorinstitution,thusreducingthemodel’s\ncan perform at a reasonable level. Gathering such domain- effectiveness in real-world clinical settings. Federated Multi-\nspecific data might be impractical because of the associated Task learning (FMTL) [6] provides a promising framework\ncost of data collection in addition to privacy concerns that that can be leveraged to solve the issue of heterogeneous\nmight arise. clients. Several approaches have been proposed to adapt MTL\nFederated learning (FL) [3] is one technique that can be tomodelFLproblems.Forexample,in[7],theauthorsaligned\nleveraged to solve such a problem without revealing local the locally learned model and the global one using regular-\ndata. The idea is to enable different clients to train a model ization, improving both model personalization and fairness\namongclients.Theexistenceofahiddenrelationshipbetween\nReceived 7 November 2024; revised 8 January 2025; accepted 31\nclients’ models was assumed in [8], and the Laplacian matrix\nJanuary 2025. Date of publication 7 February 2025; date of current version\n14October2025.ThisworkwassupportedbytheEuropeanUnionthrough was leveraged to capture this relationship.\nthe Project 6G-INTENSE under Grant 101139266. The associate editor The second issue is concerned with the computation and\ncoordinatingthereviewofthisarticleandapprovingitforpublicationwasJ.\ncommunication costs associated with LLMs. An LLM is, at\nWang.(Correspondingauthor:AhmedElbakary.)\nAhmed Elbakary, Chaouki Ben Issaid, and Mehdi Bennis are with itscore,averydeepneuralnetwork,withmillionsofweights.\nthe Centre for Wireless Communications, University of Oulu, 90014 Most state-of-the-art techniques for training neural networks\nOulu, Finland (e-mail: ahmed.elbakary@oulu.fi; chaouki.benissaid@oulu.fi;\nutilizegradient-basedmethodslikestochasticgradientdescent\nmehdi.bennis@oulu.fi).\nTamerElBattiswiththeDepartmentofComputerScienceandEngineering, (SGD). An SGD step requires computing the gradient of the\nAmerican University in Cairo, New Cairo City 11835, Egypt, and also\nloss function with respect to (w.r.t) the model’s parameters\nwiththeDepartmentofElectronicsandCommunicationsEngineering,Cairo\nUniversity,Giza12613,Egypt(e-mail:tamer.elbatt@aucegypt.edu). using BP. However, BP suffers from one major bottleneck,\nKarimSeddikiswiththeDepartmentofElectronicsandCommunications whichisthememoryfootprintneededtocomputethegradient.\nEngineering, American University in Cairo, New Cairo City 11835, Egypt\nAlong with storing models’ parameters, gradient-based meth-\n(e-mail:kseddik@aucegypt.edu).\nDigitalObjectIdentifier10.1109/LNET.2025.3539810 odsrequiretheactivationofindividualneuronsinthenetwork\n(cid:2)c 2025TheAuthors. ThisworkislicensedunderaCreativeCommonsAttribution4.0License.\nFormoreinformation,seehttps://creativecommons.org/licenses/by/4.0/\n172 IEEENETWORKINGLETTERS,VOL.7,NO.3,SEPTEMBER2025\nto be stored, enabling the optimizer to compute a backward\npass, which is needed to compute the gradient and the SGD\nupdate. This comes with almost a minimal cost in the case\nof shallow or deep networks but not very deep networks like\nLLMs. In fact, in the case of a billion-sized network, like an\nLLM, storing both the model’s parameters and the activation\nvalues would imply that tens of gigabytes are needed for\nsuch a model. Since those kinds of models are usually trained\nusing graphics processing units (GPUs), the cost might be\nquite high for even a very limited training time and might\ntake days or weeks unless a very powerful GPU cluster is\navailable.PEFTmethodssolvethisproblembyaddingasmall\nsetofparameterstothepre-trainedmodelandonlytrainingthe\nnewly added parameters. Low-Rank Adaptation (LoRA) [9]\nis one technique that enables such decomposition, where the\noriginal weight matrix is decomposed into two sub-matrices.\nThe two matrices are then initialized with Gaussian and zero\ninitialization,respectively.Duringtraining,theoriginalweight\nmatrix is frozen, while LoRA’s weights are the only trainable\nparameterstobefine-tuned.Thisfine-tuningtechniqueensures\nthat the stored activation values and the optimizer’s state are\nminimal in comparison to that of the original model. We Fig. 1. A schematic illustration of the FMTL framework for fine-tuning\nLLMs.Eachclientisequippedwiththesamepre-trainedmodelandadifferent\nleverage both the FMTL paradigm and PEFT and apply them\ntask.\nto federate the process of finetuning LLMs. The contributions\nof this letter are multi-fold: (cid:2)\n• We propose MIRA, an FMTL-based approach for fine- neighboring clients connections where δ k = (cid:2)∈N a k(cid:2). The\nk\ntuningLLMsinafederatedmannertoaddresstheissueof Laplacian matrix of the graph is defined as L = D −M,\ndata heterogeneity among clients and enable each client and its extended version as L = L⊗I d, where ⊗ denotes\nto learn a custom model tailored to its data distribution. the Kronecker product of two matrices and I d is the identity\n• We leverage a PEFT mechanism, specifically LoRA, to matrix. The FMTL problem aims to minimize the overall\nsignificantly reduce the computational and communica- objective function, which comprises the global loss and a\ntion overhead associated with LLMs, ensuring efficient regularization term enforcing task similarity as follows\nand scalable fine-tuning.\nminJ(W)=F(W)+λR(W), (1)\n• Extensive evaluation is carried out using two different W\nLLMs: Data-juicer [10] and GPT2-large [1], and two whereλisaregularizationhyperparameterwhichcontrolshow\ndatasets: Natural Instructions (NI) [11] and Dolly15-\nmuch weight a client gives to other clients’ models. In other\nk[12],toshowtheeffectivenessoftheproposedscheme. words,ifλ=0,theproblemisconvertedintoatraditionalFL\nproblem with full local training and no collaboration between\nclients is taken into account. On the other hand if λ>0, we\nII. SYSTEMMODELANDPROBLEMFORMULATION\nencourage similar tasks/clients to align their models close to\nWeassumeanetworkofK clients,withthesetofallclients each other. The global loss F(W) is defined as the sum of\ndefinedasN ={1,...,K}andN\nk\n=N\\{k}beingthesetof\nlocal losses across all clients\nall neighboring clients for client k. Clients communicate only\n(cid:3)K\nwiththeparameterserver(PS),butnotwitheachotherdirectly.\nEach clienttrainsaseparate modelw k ∈Rd ,according toits F(W)= f k(w k). (2)\nlocal data (X k ,y k), where X k represents the feature matrix, k=1\nthe instruction to the LLM in our case, and y k represents the The Laplacian regularization term R(W) captures the\nlabel vectors, the expected output from the LLM. Along with similarity between clients’ task models and is defined as\nthat, clients have their own tasks t k and objective function (cid:3)N (cid:3)\nf k(w k):Rd −→R,∀k ∈N.ThesettingisdepictedinFigure 1. R(W)=WTLW = 1 a k(cid:2) (cid:6)w k −w (cid:2) (cid:6)2. (3)\nWemodeltheinteractionsamongclientsasaconnectedgraph 2\nk=1(cid:2)∈N\nG, with an adjacency matrix M that quantifies the similarity k\nbetweentasksofdifferentclients.TheentriesofthematrixM In this formulation, f k(w k) represents the local loss function\n(i)\nrepresent how strong the relationship is between two clients, at client k, computed by evaluating a sample x using the\nk\ni.e., the degree of similarity between the two tasks. model w k. The norm (cid:6)·(cid:6) denotes the Euclidean norm, and\nLet W =[wT 1 ,...,wT K ]∈RdK be a concatenation of all a k(cid:2) quantifies the similarity between tasks of clients k and (cid:4).\nclients’ models and let D = diag[δ 1 ,δ 2 ,...,δ K] ∈ RK×K Inotherwords,withhighervaluesofa k,(cid:2),itindicatesstronger\nbe a diagonal matrix that represents the summation of all relationships. The objective function J(W) consists of two\nELBAKARYetal.:MIRAMODELS 173\naddresses the memory footprint issue by reducing the number\nof trainable parameters in the model. We assume that each\nclientisequippedwithapre-trainedmodelW0 ∈Rd×v\n.This\nweight matrix is then decomposed into two smaller matrices\nsuch that\nW0 +ΔW =W0 +BA, (5)\nwhereΔW representstheaccumulatedgradientupdates,B ∈\nRd×r and A ∈ Rr×v , where r being the rank of both A,\nand B. Instead of optimizing the original weights W0 , we\nonly update A and B. LoRA hypothesizes that projecting\nLLMs into lower-dimensional spaces preserves performance,\nallowing us to optimize these smaller matrices efficiently. In\nthe rest of this letter, we refer to B and A as ΔW.\nFig.2. Globalmodelversusper-taskmodelperformance. At communication round t, the server uniformly selects a\nsubset of clients S(t) . Each client, k ∈S(t) , performs R local\nrounds of updates on the lower-dimensional matrix, ΔW k,\ncomponents: the global loss, which is optimized locally by\nsuch that\neach client, and the regularization term, which is handled by (cid:4) (cid:5)\n(t) (t)\nthe server. To minimize J(W), we perform both local and ΔW k,R =InstructionTuning ΔW k,r ,r ∈{1,...,R},\nglobalupdatesiteratively.Ateachclientk,weperformRlocal\n(6)\noptimization steps to update w k. After completing the local\nupdates, the server minimizes the regularization term R(W) where InstructionTuning(·) refers to the process of adapting\nby adjusting the clients’ models based on their similarities. the weight matrix to the new dataset by computing a forward\nSpecifically, the server updates each client’s model as follows pass, and a backward pass, and then updating the model’s\n(cid:3) (cid:4) (cid:5) weights. By leveraging LoRA, our forward pass becomes\n(t+1) (t) (t) (t)\nw\nk\n=w\nk,R\n−ηλ a\nk(cid:2)\nw\nk,R\n−w\n(cid:2),R\n,∀k ∈N,\nh =W0x +BAx, (7)\n(cid:2)∈N\nk\n(4) where h is the output of the forward pass, and x is a sample\n(t) drawn from the dataset which is of the form of instructions\ns w te h p e s re at w it k e , r R ati i o s n th t e , a l n o d ca η lly is u t p h d e at s e e d rv m er o ’s de le l a o rn f i c n l g ie r n a t te k . after R X k and the expected response from the model y k. After\nthe forward pass, a backward pass is calculated with BP,\nTo motivate the FMTL setting, we conduct a prelimi-\ncomputing the gradient w.r.t the LoRA’s weights. Finally, we\nnary experiment with the DataJuicer LLM [10] on the NI\nupdate the model weights using an optimizer step, yielding\ndataset[11].Wemeasuretheperformanceofthegloballearned\na locally updated model. This process continues for R local\nmodel on different tasks by reporting the average loss per (t)\nrounds. The final locally updated model ΔW is sent to\ntaskusingoneofthelatestproposedmethods,FedKSeed[13] k,R\nthe server. The server then performs a regularization update\nagainst learning a fully local model. As we can see from\nto align clients with similar tasks, encouraging their models\nFigure 2, the performance of the model varies across tasks.\nto be closer to each other. The server updates each client’s\nMost importantly, the global model’s performance is worse matrix, ∀k ∈S(t) , as follows\nthan the local ones in certain tasks with a high variance in\n(cid:2) (cid:3) (cid:4)\n(t+1) (t) (t) (t)\nthe case where the size of the local dataset is small. This can ΔW =ΔW −ηλ a ΔW −ΔW .\nk k,R k(cid:2) k,R (cid:2),R\nbe attributed to the fact that learning a single global model, (cid:2)∈N\nk\nas in the traditional FL approach, is sub-optimal in the case (8)\nof different tasks or data distributions across clients. On the\nother hand, learning a fully local model is near optimal in It is worth noting that the performance of the traditional\ncase sufficient compute and data are available, which is not aggregation scheme that averages the model"
  },
  "Privacy-Aware_Split_Federated_Learning_for_LLM_Fine-Tuning_over_Internet_of_Things (1).pdf": {
    "pages": 12,
    "content": "This article has been accepted for publication in IEEE Internet of Things Journal. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/JIOT.2025.3600269\n1\nPrivacy-Aware Split Federated Learning for LLM\nFine-Tuning over Internet of Things\nXiaopei Chen, Student Member, IEEE, Wen Wu, Senior Member, IEEE, Fei Ji, Member, IEEE,\nYongguang Lu, and Liang Li, Member, IEEE\nAbstract—The proliferation of Internet of Things (IoT)- billion[2],generatingrich,real-time,anddomain-specificdata\ngenerated distributed personal data enables user-specific large streams [3]. These IoT devices offer rich and diverse data\nlanguagemodel(LLM)adaptationattheedge.Thesplitfederated\nto support LLM development, while LLMs are expected to\nlearning (SFL) facilitates collaborative learning and reduces\nenhance the intelligence of IoT [4–6]. This convergence of\nmemory footprint by model splitting, which necessitates the\ntransmission of intermediate activations, rendering it suscep- IoT and LLM technologies thus creates unprecedented oppor-\ntible to reconstruction attacks and privacy breaches. In this tunities to advance smart services and connected applications.\npaper, we present a privacy-aware SFL scheme addressing the As IoT devices persistently generate decentralized data with\naccuracy-efficiency-privacy trilemma in LLM fine-tuning over\nprivacy implications [7, 8], there is a critical requirement to\nheterogeneous IoT devices. Particularly, we develop a privacy\nadapt LLMs through localized fine-tuning to meet specialized\nquantification metric based on Fisher information to assess\nlayer-wise privacy risks in smashed data transmission. Guided service needs while ensuring information privacy.\nby this metric, we establish an analytical model that captures Split federated learning (SFL) offers an efficient frame-\nthe intricate relationships between privacy leakage, fine-tuning work for fine-tuning LLMs in IoT networks by harnessing\nconvergence time, and device energy consumption. To optimize\ndistributed device data while overcoming memory limitations\nthesethreeaspects,weformulateamulti-objectivemixed-integer\n[9, 10]. Unlike conventional federated learning (FL) that\nprogramming problem. Then, an (cid:15)-constraint-based block coor-\ndinatedescent(BCD)algorithmisproposedtojointlydetermine requires storing the whole model and processing on each\nthe optimal LLM split layer, transmit power, and bandwidth device [11], SFL divides the LLM into client-side and server-\nallocationforIoTdevicesundertheirmemoryandnetworkcon- side submodels [12]. Each IoT device exclusively trains its\nstraints. Extensive simulation results demonstrate the proposed\nassignedclientsubmodel,dramaticallyreducinglocalmemory\nscheme’seffectivenessinachieving24%fasterconvergence,40%\nand processing requiremets, making it particularly suitable\nlower energy consumption, and 7% reduced privacy leakage\ncomparedtobaselineapproaches,whilemaintainingcompetitive for resource-constrained IoT devices. In the SFL scheme, the\nmodel accuracy. devices locally update their submodels and the edge server\nupdates the corresponding submodels, where the intermediate\nIndex Terms—LLM fine-tuning, split federated learning, pri-\nvacy leakage, multi-objective optimization. outputs (smashed data) and the gradients at the split layer\nare exchanged between the device and the server. A critical\nchallenge lies in optimizing model split strategies to accom-\nI. INTRODUCTION modate devices with varying computational capacities and\nThe rapid advancement of artificial intelligence (AI) has memory resources. This directly impacts resource efficiency,\ndriven large language models (LLMs), such as GPT and fine-tunedmodelaccuracy,andthesystem’sabilitytomitigate\nLLaMa, to achieve remarkable progress in natural language performance bottlenecks caused by slower devices.\nprocessingandintelligentdecision-making.Thesemodelsrely In the literature, several pioneering studies [13–18] have\non large-scale, diverse data for continual improvement [1]. explored integrating SFL with parameter-efficient fine-tuning\nMeanwhile, with the rapid growth of the Internet of Things (PEFT)methodslikelow-rankadaptation(LoRA)[19],where\n(IoT),thenumberofglobalIoTdevicesisexpectedtobe3.56 only minimal activated parameters (rather than full model\nparameters) undergo updates during training. Existing works\nThis work was supported in part by the Pengcheng Laboratory Major primarily focus on optimizing training latency [13–15] and\nKey Project under Grant 2025QYA002 and 2025QYB041, in part by the energy consumption [15–17] through strategic split layer\nNaturalScienceFoundationofChinaunderGrant62201071,62201311,and\nselection, computation resource scheduling, and communi-\n62192712.(Correspondingauthor:WenWu.)\nXiaopei Chen is with the School of Future Technology, South China cation resource allocation. The core challenge resides in\nUniversity of Technology, Guangzhou 511442, China, and also with the balancing training performance against device resource con-\nDepartmentofStrategicandAdvancedInterdisciplinaryResearch,Pengcheng\nstraints—shallowersplitlayer reducesclient-sidecomputation\nLaboratory,Shenzhen518000,China(e-mail:ftchenxp@mail.scut.edu.cn).\nWenWuandLiangLiarewiththeDepartmentofStrategicandAdvanced at the cost of increased resource conflict at the edge server,\nInterdisciplinary Research, Pengcheng Laboratory, Shenzhen 518000, China while deeper split layer alleviates server-side load but im-\n(e-mail:{wuw02,lil03}@pcl.ac.cn).\nposeheaviercomputationalandmemoryburdensonresource-\nFei Ji is with the School of Electronic and Information Engineering,\nSouth China University of Technology, Guangzhou 510640, China (e-mail: constrained IoT devices.\neefeiji@scut.edu.cn). However, critical limitations persist in current SFL-based\nYongguang Lu is with the School of Computer Science and En-\nLLM fine-tuning frameworks. Firstly, most existing ap-\ngineering, Sun Yat-sen University, Guangzhou 510006, China (e-mail:\nluyg5@mail2.sysu.edu.cn). proaches neglect the inherent privacy vulnerabilities in client-\nAuthorized licensed use limited to: Universite du Quebec en Outaouais. Downloaded on November 19,2025 at 22:06:50 UTC from IEEE Xplore. Restrictions apply.\n© 2025 IEEE. All rights reserved, including rights for text and data mining and training of artificial intelligence and similar technologies. Personal use is permitted,\nbut republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.\nThis article has been accepted for publication in IEEE Internet of Things Journal. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/JIOT.2025.3600269\n2\nserver smashed data exchanges. LLM fine-tuning exhibits model is presented. The problem formulation and transfor-\nthe “not-too-far” property [20], meaning that the pre-trained mation are given in Section IV. Section V proposes the\nmodel features remain largely preserved after fine-tuning. optimization algorithms. The simulation results are shown in\nThis property makes it easier for adversaries to leverage pre- Section VI. Section VII concludes this work.\ntraining knowledge to perform data reconstruction attacks,\nespecially when intermediate data transmissions are not pro- II. RELATEDWORK\ntected. Secondly, split layer configuration creates a complex\nA. LLM Fine-Tuning over IoT Devices\ntripartitetrade-off:Shallowsplitlayer(closertoinput)exposes\nRecentadvancesinPEFThaveenabledpreliminaryattempts\nmorerawdatapatternsinsmashedoutputs,increasingprivacy\nto fine-tune LLMs on resource-constrained IoT devices. In\nleakagesusceptibility,whiledeepersplitlayershiftscomputa-\n[21], a split federated LoRA framework is proposed for LLM\ntional burdens to resource-constrained devices. Furthermore,\nfine-tuning by integrating LoRA with FL, and an online\nprivacy-enhancing techniques like gradient noise injection,\nalgorithm for effective device scheduling and bandwidth allo-\nthough effective for shallow split layer, disproportionately\ncation is developed to enhance learning performance. In [22],\ndegrade model convergence compared to their application in\na device-edge cooperative fine-tuning paradigm is proposed\ndeeper split layer. Thirdly, the heterogeneous nature of device\nto fine-tune different layers of an LLM, and the bandwidth\ncapabilities and communication conditions compounds these\nand layer are jointly allocated to minimize the training time.\nchallenges. Coordinating privacy preservation, training effi-\nIn [23], a sparsely activated LoRA algorithm that freezes\nciency, and energy conservation across devices with varying\nthe pre-trained foundation model parameters and inserts low-\ncapabilities forms a non-trivial multi-objective optimization\nrank adaptation matrices into transformer blocks is designed\nproblem. These interdependent constraints resist conventional\nto adapt to the limited computational resources. These ap-\noptimization approaches, particularly when managing diverse\nproaches maintain a full pre-trained LLM locally for fine-\nIoT nodes with disparate resource profiles and privacy thresh-\ntuning. This demands significant device memory, creating a\nolds.\ncritical bottleneck for resource-constrained IoT devices that\nIn this paper, we propose a privacy-aware SFL scheme for\nlack the capacity to store and execute full-scale LLMs.\nLLMfine-tuningoverheterogeneousIoTdevices.Specifically,\nTo reduce local computation and memory usage, SL and\nwe develop a privacy quantification metric based on Fisher\nSFL have been explored to distribute the fine-tuning process.\ninformation to assess layer-wise privacy risks in smash data\n[9]and[10]proposethefullfine-tuning-basedSFLandLoRA\ntransmissions. Guided by it, we establish analytical mod-\nfine-tuning-basedSFL,respectively.SincetheLoRAapproach\nels that capture the intricate relationships between privacy\ncan reduce computational resource consumption while main-\nleakage, fine-tuning convergence time, and device energy\ntaining model performance. Subsequent studies have incorpo-\nconsumption.Tooptimizetheefficiency-privacytrade-offs,we\nrated LoRA-based fine-tuning into SL or SFL architectures\nformulateamulti-objectivemixed-integerprogramming(MIP)\nto facilitate further research [14–17]. In [16], a time and\nproblem that aims to minimize these three performance met-\nmemory-efficient collaborative split fine-tuning framework is\nrics. An (cid:15)-constraint-based block coordinate descent (BCD)\nproposedtobreakdowntheresourcewalloffine-tuningLLMs\nmethod is proposed to jointly determine optimal split layer,\nat individual devices, where the training time is minimized\ntransmit power, and bandwidth allocation for IoT devices\nby optimizing the split layer selection and data and pipeline\nunder their memory and network constraints. Specifically,\nparallelization design under the memory constraint. In [14],\nthe (cid:15)-constraint method is employed to transform the multi-\nunderanSLarchitecture,splitlayerselectionandcomputation\nobjectiveoptimizationproblemintoasingle-objectiveproblem\nresourcesareoptimizedtominimizetrainingdelayandenergy\nwhile ensuring weak Pareto optimality. Leveraging the BCD\nconsumption. In [17], a pipeline parallel training mechanism\nmethod, the problem is decomposed into three subproblems.\nis developed to ensure fast and efficient distributed training,\nThe optimal bandwidth allocation is first derived through\nwheresplitlayerselectionandbackendscheduleraredesigned\nmonotonicityanalysis.Next,thetransmitpowerisdetermined\nto minimize memory usage and training delay. In [15], a\nvia a bisection method. Finally, the optimal split layer is\nLoRA fine-tuning-based SFL is considered, and the total\nobtained using a successive convex approximation (SCA)-\nenergy consumption and training delay are minimized by\nbased approach. The main contributions of this paper are as\noptimizing the split layer selection, communication resource,\nfollows:\nand computation resource. Most existing studies primarily\n• We establish a unified model for SFL-based LLM fine- focusonimprovingenergyefficiencyandtraininglatency,with\ntuningthatquantifiesprivacyrisk,memoryfootprint,and limited attention to potential privacy risks during client-server\nenergy consumption. interactions.\n• We formulate a multi-objective mixed-integer program-\nming problem by optimizing split layer, transmit power, B. Privacy Leakage in Split Learning\nand bandwidth allocation.\nPrivacyleakagehasbeennoticedinSLfortraditionalDNN\n• We design an efficient (cid:15)-constraint-based BCD algorithm\nmodels. In [24], a privacy-aware adaptive model splitting\nto solve the multi-objective MIP problem.\napproach is proposed to balance privacy preservation and\nThe remainder of this paper is organized as follows. Sec- communication-computation performance, where the struc-\ntion II presents the related works. In Section III, the system turalsimilarityindexmeasure(SSIM)isusedasanevaluation\nAuthorized licensed use limited to: Universite du Quebec en Outaouais. Downloaded on November 19,2025 at 22:06:50 UTC from IEEE Xplore. Restrictions apply.\n© 2025 IEEE. All rights reserved, including rights for text and data mining and training of artificial intelligence and similar technologies. Personal use is permitted,\nbut republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.\nThis article has been accepted for publication in IEEE Internet of Things Journal. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/JIOT.2025.3600269\n3\nTABLE I: Summary of Notations\nSymbol Description\nD\nu\nLocaldatasetofclientu\nC\nu\nComputingcapabilityofclientu\nM\nu\nAvailablememoryofclientu\nC s Computingcapabilityoftheedgeserver\nWu Clientsub-modelofclientu\nc\nRu LoRAmoduleonclientu\nc\nWu Serversub-modelforclientu\ns\nRu LoRAmoduleontheserverforclientu\ns\nl\nu\nSelectedsplitlayerofclientu\nψ\nu\nPrivacyleakageofclientu\nB\nu\nBandwidthallocatedtoclientu\nP\nu\nTransmitpowerofclientu\nP s Transmitpoweroftheserver\nh\nu\nChannelgain(clientutoserver)\nn 0 Noisepowerspectraldensity\nFig. 1. System model.\nB,S,H Batchsize,sequencelength,hiddendimension\ntu u p,td u own Upload/downloaddelayofclientu\ntc u omp Localcomputationdelayofclientu\nmetric for data privacy. In [25], a hybrid federated split ts u omp Servercomputationdelayforclientu\nlearning architecture that combines efficiency and privacy Eu comp Computationenergyconsumptionofclientu\nis suggested, where the privacy leakage is evaluated by an Ecom Communicationenergyconsumptionofclientu\nu\nattacker encoder model. In [2], DNN model partitioning\nand resource allocation are optimized to reduce the energy\nconsumption under the amount of privacy leakage, where\nThe system operates under an SFL framework that"
  },
  "SplitLoRA.pdf": {
    "pages": 9,
    "content": "JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2015 1\nSplitLoRA: A Split Parameter-Efficient Fine-Tuning\nFramework for Large Language Models\nZheng Lin, Xuanjie Hu, Yuxin Zhang, Zhe Chen, Member, IEEE, Zihan Fang, Xianhao Chen, Member, IEEE,\nAng Li, Member, IEEE, Praneeth Vepakomma, Member, IEEE, and Yue Gao, Fellow, IEEE\nAbstract—The scalability of large language models (LLMs) in smart healthcare [4], [5], computer vision [6]–[8], intelligent\nhandlinghigh-complexitymodelsandlarge-scaledatasetshasled transportation [9]–[11] due to their exceptional capabilities in\nto tremendous successes in pivotal domains. While there is an\nhandlinghigh-complexitymodelsandlarge-scaledatasets[1]–\nurgentneedtoacquiremoretrainingdataforLLMs,aconcern-\n[3], [12]. However, a critical concern accompanying the surge\ning reality is the depletion of high-quality public datasets within\na few years. In view of this, the federated learning (FL) LLM of LLMs has emerged: it is estimated that high-quality public\nfine-tuning paradigm recently has been proposed to facilitate datasets will be depleted before 2026 [13]. The increasing\ncollaborativeLLMfine-tuningondistributedprivatedata,where trend of researchers preferring to train data-hungry LLMs by\nmultiple data owners collaboratively fine-tune a shared LLM combining existing datasets [14] or utilizing model-generated\nwithout sharing raw data. However, the staggering model size\ndatasets[15],ratherthancollectingorgeneratingnewdataset,\nof LLMs imposes heavy computing and communication burdens\non clients, posing significant barriers to the democratization of also reflects the current scarcity of public available data. This\nthe FL LLM fine-tuning paradigm. To address this issue, split suggeststhatthedevelopmentofcurrentLLMsmayencounter\nlearning (SL) has emerged as a promising solution by offloading significant bottlenecks shortly, as the well-established scaling\ntheprimarytrainingworkloadtoaserverviamodelpartitioning\nlaws indicate that larger datasets usually lead to better perfor-\nwhile exchanging activation/activation’s gradients with smaller\nmance [16].\ndata sizes rather than the entire LLM. Unfortunately, research\nontheSLLLMfine-tuningparadigmisstillinitsnascentstage. With the rapid proliferation of Internet of Things (IoT)\nTo fill this gap, in this paper, we propose the first SL LLM devices and advancements in sensor technology, connected\nfine-tuningframework,namedSplitLoRA.SplitLoRAisbuilton IoT devices are capable of collecting massive data. However,\nthesplitfederatedlearning(SFL)framework,amalgamatingthe these high-quality data distributed across multiple parties\nadvantagesofparalleltrainingfromFLandmodelsplittingfrom\ncannot be shared publicly due to issues such as user privacy\nSLandthusgreatlyenhancingthetrainingefficiency.Itisworth\nnoting that SplitLoRA is the inaugural open-source benchmark concerns(e.g.,medical[17]andfinancial[18]data)orphysical\nfor SL LLM fine-tuning, providing a foundation for research limitations (e.g., lack of network connectivity). Some leading\nefforts dedicated to advancing SL LLM fine-tuning. Extensive AI technology companies can gather large volumes of private\nsimulations validate that SplitLoRA achieves target accuracy\ndatatotraindata-hungryLLMs,exemplifiedbyGoogle’sMed-\nin significantly less time than state-of-the-art LLM fine-tuning\nPaLM [19], a medical LLM capable of providing expert-level\nframeworks, demonstrating the superior training performance\nof SplitLoRA. The project page is available at https://fdu- medicalguidanceanddiagnosis.Nevertheless,noteveryparty\ninc.github.io/splitlora/. possesses adequate private data to independently train a high-\nperforming and data-hungry LLM. Considering the increasing\nIndex Terms—Distributed learning, split federated learning,\nclient-side model aggregation, model splitting, mobile edge com- scarcity of public data and the difficulties in accessing user-\nputing. private data, devising collaborative training paradigms for\ndecentralized private data without data sharing is paramount\nI. INTRODUCTION\nto driving the advancement of modern LLMs.\nIn recent years, LLMs [1]–[3] have achieved tremendous The federated learning (FL) LLM training/fine-tuning\nsuccesses across a broad spectrum of pivotal domains such as paradigm [20]–[22] has recently been proposed to facilitate\ncollaborative LLM training on distributed private data, where\nZ. Lin, X. Hu, Y. Zhang, Z. Chen, Z. Fang and Y. Gao are multiple data owners collaboratively training a shared LLM\nwith the School of Computer Science, Fudan University, Shanghai without sharing raw data. In FL LLM paradigm, data owners\n200438, China (e-mail: zlin20@fudan.edu.cn; xjhu23@m.fudan.edu.cn;\ntrain local LLMs on their respective local data and then send\nyuxinzhang22@m.fudan.edu.cn; zhechen@fudan.edu.cn; zh-\nfang19@fudan.edu.cn; gao.yue@fudan.edu.cn). Z. Lin is also with the the LLM parameters rather than raw data to a parameter\nDepartment of Electrical and Electronic Engineering, University of Hong server for model update [23]–[25]. However, the staggering\nKong,PokFuLam,HongKong,China.\ngrowth in LLM model sizes imposes heavy computing and\nX. Chen is with the Department of Electrical and Electronic Engineer-\ning, University of Hong Kong, Pok Fu Lam, Hong Kong, China (e-mail: communication burdens, posing significant barriers to the de-\nxchen@eee.hku.hk). mocratizationoftheFLLLMparadigm.Toaddressthisissue,\nA. Li is with the Department of Electrical and Computer Engineer-\nsplitlearning(SL)[26]hasemergedasapromisingdistributed\ning, University of Maryland, College Park, MD-20742, USA (e-mail: an-\ngliece@umd.edu). MLframeworkcapableofovercomingtheweaknessofFLby\nP.VepakommaiswithMohamedbinZayedUniversityofArtificialIntelli- offloadingtheprimarytrainingworkloadtoaserverviamodel\ngence,AbuDhabi,UnitedArabEmirates,andtheMassachusettsInstituteof\npartitioning while exchanging activation/activation’s gradients\nTechnology,Cambridge,MA02139USA(e-mail:vepakom@mit.edu).\n(Correspondingauthor:YueGao) with smaller data sizes rather than the entire LLM [27]–[29].\n4202\nluJ\n1\n]GL.sc[\n1v25900.7042:viXra\nJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2015 2\nUnfortunately, research on the SL LLM fine-tuning paradigm strating that SplitLoRA is more communication- and\nis still unexplored. computation-efficient.\nTo fill this gap, in this paper, we propose a concise The remainder of this paper is organized as follows. Sec-\nand research-friendly SL LLM fine-tuning framework, named tion II introduces related work. Section III elaborates on\nSplitLoRA. SplitLoRA is built on the split federated learning systemmodelandSplitLoRAframework.SectionIVprovides\n(SFL) [30] framework that amalgamates the advantages of the simulation results. Section V discusses the future research\nparallel training from FL and model splitting from SL and is direction. Finally, concluding remarks are presented in Sec-\ndeveloped on the well-known parameter-efficient fine-tuning tion VI.\n(PEFT) technique Low-rank adaptation (LoRA) [31], [32].\nII. RELATEDWORK\nSpecifically, we start from pre-trained LLMs on a large-\nscale dataset and then train/fine-tune the LLMs to adapt the A. Large Language Models\ndownstream tasks via SFL, which consists of several stages:\nDriven by the maturation of deep learning algorithms,\nclient-side forward propagation (FP), activations transmis-\nincreasedcomputingcapabilities,andtheavailabilityoflarge-\nsions, server-side FP and back-propagation (BP), activations’\nscale datasets, LLMs have made significant strides across\ngradients transmissions, client-side BP, and client-side model\nindustries. Major players in the AI community [1]–[3], [35]–\naggregation. For experimental configurations, we 1) imple-\n[37], including OpenAI, Google, Microsoft, and Facebook,\nment SplitLoRA framework based on GPT-2 [33] on the\nare dedicated to developing their own LLMs. For instance,\nE2E dataset [34] and evaluate its performance across diverse\nOpenAI’s highly acclaimed chat LLM, GPT series [1], [35],\nperformance metrics (e.g., BLEU, METEOR, and NIST); 2)\n[36], and Google’s BERT [37] have demonstrated superior\nadopt the SFL framework, amalgamating the advantages of\nperformance in a wide spectrum of Natural Language Pro-\nparallel training from FL and model splitting from SL and\ncessing (NLP) tasks like language translation, text generation,\nthus greatly enhancing the training efficiency. 3) employ the\nquestion answering [38], and sentiment analysis [39], [40].\nPEFT technique, LoRA, enabling training to be executed on\nMoreover,LLMhasextendedbeyonditsoriginalNLPdomain\na single consumer-grade GPU (such as the NVIDIA 3090).\nto shine in pivotal domains including healthcare [4], [19],\nBesides,weinvestigatethepotentialofSplitLoRAforpractical\nautonomous driving [41], [42], and robotic control [43], [44].\ndeployments by comparing it with state-of-the-art LLM fine-\nFor example, in healthcare, Med-PaLM [19] is devised for\ntuning paradigm in terms of converged accuracy, convergence\nmedical image analysis, clinical document processing, and\nrate,andresourceefficiency.ItisworthnotingthatSplitLoRA\npatient diagnosis, facilitating accurate diagnoses and treat-\nis the inaugural open-source benchmark for SL LLM fine-\nment decisions of healthcare professionals. In the realm of\ntuning, providing a foundation for research efforts dedicated\nautonomousdriving,DriveMLM[41]bridgesthegapbetween\nto advancing SL LLM fine-tuning.\nlanguage decisions and vehicle control commands, enabling\nIn the near future, we anticipate that others will build\nclosed-loop autonomous driving in realistic simulators. As\nuponourSplitLoRAframeworkforfurtherexplorations.There\nthe scale of LLM models substantially increases, they exhibit\nare several reasons for this: (1) Some new challenges and\nexceptionalgeneralizationcapabilities—aphenomenonknown\ndirections are emerging in the SL LLM fine-tuning, such as\nas“emergence”[5].Oneofthemostrepresentativeexamplesis\ndetermining optimal model splitting for LLMs and extension\nGPT-4 [36], due to its staggering model size, can successfully\nofSLLLMparadigmtoaccommodateheterogeneouscomput-\nperformarithmeticoperationssuchasnumericalmultiplication\ningresourcesacrossclients.(2)TailoringtheSLLLMtraining\nwithoutspecifictargetedtraining.Theoutstandingcapabilities\nframework specifically to different application scenarios, e.g.,\nof LLMs enable them to be directly applied or easily adapted\nvehicular networks and satellite networks, etc., to achieve\n(e.g., through fine-tuning or instruction adjustment) to various\nhighertrainingefficiency.(3)InthiseraofLLMs,weadvocate\ndownstream or novel tasks, thereby unleashing unprecedented\nfuture work in the SL LLM domain to modify our framework\npotentialinapplicationssuchaschatbots,healthcareassistants,\nand assess the performance of their algorithms.\nand intelligent transportation [1], [5], [41], [45]–[47].\nOur contributions are as follows:\nB. Split Learning\n• We explore the pipeline for fine-tuning contemporary\nLLMs on decentralized private data resources via SFL, Split learning (SL) [26] has emerged as a promising dis-\npointing out a promising development direction for tributed ML framework, capable of overcoming the weak-\nLLMs. nesses of FL [24], [48]. SL offloads the primary training\n• Weproposeaconciseandresearch-friendlySLLLMfine- workload to the server via model partitioning [29], [30], [49],\ntuning framework named SplitLoRA. It is worth noting whileexchangingactivation/activation’sgradientswithsmaller\nthat SplitLoRA is the inaugural open-source benchmark data sizes rather than the entire model, thereby significantly\nfor SL LLM fine-tuning, providing a foundation for reducingclient-sidecomputingcostsandcommunicationover-\nresearch efforts dedicated to advancing SL LLM fine- head during model training [28], [29]. However, the original\ntuning. SL, entitled vanilla SL [26], employs a sequential training\n• We conduct extensive experiments to compare Split- mechanism from one device to another, substantially reducing\nLoRA with the conventional centralized and federated training efficiency and resulting in excessive training latency.\nLLM fine-tuning paradigms in terms of converged accu- To address this issue, split federated learning (SFL) [30]\nracy, convergence rate, and resource efficiency, demon- has been proposed, which merges the advantages of parallel\nJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2015 3\ntraining from FL and model splitting from SL. Apart from\na1\nmodel splitting, SFL features periodic server-side and client- SplitLoRA\nside sub-model aggregation to achieve model synchroniza- Local aggregation\nserver a2\ntion after multiple training rounds, aligning with the design b1 Client server 1 a3\nprinciple of FL [27], [29]. Due to its salient advantages,\nSFL has garnered significant attention from academia and\nindustryinrecentyears.Inacademia,thepredominantresearch Every I\nrounds Client server 2 Central server\nefforts focus on the framework design of SFL to enhance\nb2\n...\nmodel trainingefficiency, including model splitting[49], [50], b3 a4\nmodel aggregation [49], client selection [51], and device\nclustering [50], [52]. In the industry, Huawei deems SFL as a a5 Client server N\npivotal learning framework for 6G edge intelligence due to its\nflexibility and inherent advantages [53]. Client-side Aggregated client-side Server-side Client-side pre-trained Server-side pre-trained\nLoRA adapters LoRA adapters LoRA adapters model model\nThe effectiveness of SFL methods has been validated in\nimage classification and small-scale models (e.g., ResNet- a. Split fine-tuning stage: a1 - a5 (executed at each training round)\na1 Client-side model FP a2 Activations transmissions a3 Server-side model FP and BP a4 Activations’ gradients transmissions\n18 [54] andVGG-16 [55]). However, the performanceof SFL a5 Client-side model BP\nb. Client-side LoRA adapter aggregation stage: b1 - b3 (executed at every I training round)\nframeworks in the current LLM training paradigm remains b1 Client-side LoRA Adapters Uploading b2 Client-side LoRA Adapter Aggregation b3 Client-side LoRA Adapter Downlink Transmissions\nunclear. Therefore, in this study, we are the first to investigate\nthe performance of SFL frameworks in LLM training, provid- Fig. 1: Overview of proposed SplitLoRA framework.\ning novel insights into SL LLM training and establishing a\nfoundation for future research.\nD. Split Learning and Large Language Models\nC. Parameter-Efficient Fine-Tuning\nRecently, there have been several preliminary papers ex-\nThe conventional full-parameter fine-tuning paradigm (i.e., ploring FL LLM PEFT paradigms [63]–[66]. Even though\nupdatingallparameters)iscomputationallyexpensiveforfine- these frameworks have adopted various PEFT methods for\ntuning LLMs on edge devices/servers. Moreover, for dis- LLM fine-tuning, such as Adapter [56]–[58] and LoRA [31],\ntributedlearningparadigmssuchasFLandSL,full-parameter [32],however,LLMfine-tuningisstillcomputationaloverload\nfine-tuning also incurs substant"
  },
  "VFLAIR-LLM.pdf": {
    "pages": 12,
    "content": "VFLAIR-LLM: A Comprehensive Framework and Benchmark for\nSplit Learning of LLMs\nZixuanGu QiufengFan LongSun\nSchoolofSoftware,Tsinghua WuxiInnovationCenterofTsinghua WuxiInnovationCenterofTsinghua\nUniversity AIR AIR\nBeijing,China Wuxi,China Wuxi,China\ngu-zx24@mails.tsinghua.edu.cn fan.qiufeng@u.nus.edu cnlonger@gmail.com\nYangLiu∗ XiaojunYe\ntheHongKongPolytechnic SchoolofSoftware,Tsinghua\nUniversity University\nHongKong,China Beijing,China\nyang-veronica.liu@polyu.edu.hk yexj@tsinghua.edu.cn\nAbstract LLMs.InProceedingsofthe31stACMSIGKDDConferenceonKnowledge\nWiththeadvancementofLargeLanguageModels(LLMs),LLM DiscoveryandDataMiningV.2(KDD’25),August3–7,2025,Toronto,ON,\nCanada.ACM,NewYork,NY,USA,12pages.https://doi.org/10.1145/3711\napplicationshaveexpandedintoagrowingnumberoffields.How-\n896.3737411\never,userswithdataprivacyconcernsfacelimitationsindirectly\nutilizingLLMAPIs,whileprivatedeploymentsincursignificant\ncomputationaldemands.Thiscreatesasubstantialchallengein\n1 Introduction\nachievingsecureLLMadaptationunderconstrainedlocalresources.\nToaddressthisissue,collaborativelearningmethods,suchasSplit The recent development and success of large language models\nLearning (SL), offer a resource-efficient and privacy-preserving (LLMs)havesignificantlyreshapedthelandscapeofartificialintel-\nsolutionforadaptingLLMstoprivatedomains.Inthisstudy,we ligence,showcasingexceptionalcapabilitiesacrossawiderangeof\nintroduceVFLAIR-LLM(availableathttps://github.com/FLAIR- tasks.LLMtrainingreliesheavilyonmassive,high-qualitydata,fu-\nTHU/VFLAIR-LLM),anextensibleandlightweightsplitlearning elinggrowingdemandforsuchresources.Publicdatasources,such\nframeworkforLLMs,enablingprivacy-preservingLLMinference asbooks,webcrawls,andopen-accessarticles,havehistorically\nandfine-tuninginresource-constrainedenvironments.Ourlibrary servedasthebackboneofLLMtrainingdata.However,research[38]\nprovidestwoLLMpartitionsettings,supportingthreetasktypes indicatesthattheavailabilityofpublichumantextdataisnearing\nand18datasets.Inaddition,weprovidestandardmodulesforim- exhaustion.Thisgrowingdatascarcityhasemergedasacritical\nplementingandevaluatingattacksanddefenses.Webenchmark5 bottleneckforLLMdevelopment,compellingashifttowardlever-\nattacksand9defensesundervariousSplitLearningforLLM(SL- agingprivatedomaindata,whichsubsequentlyraisessignificant\nLLM)settings,offeringconcreteinsightsandrecommendationson privacyconcerns.\nthechoiceofmodelpartitionconfigurations,defensestrategies,and Sensitivedatawithinprivatedomainscannotbefreelysharedor\nrelevanthyperparametersforreal-worldapplications. processedbyexternalLLMsystemsduetorisksofdatabreaches,\nregulatoryviolations,andpotentialmisuse.Thesechallengesmake\nCCSConcepts thedirectintegrationofprivatedataintoLLMtrainingimpractical.\n•Securityandprivacy→Distributedsystemssecurity. OnepotentialsolutionisthelocaldeploymentofLLMs.However,\nthismethodrequiressubstantiallocalcomputationalresources,pos-\nKeywords ingasignificantbarrierforsmallerorganizationsorindividuals\nmanagingsensitiveprivatedata.Toaddressthischallengeofpri-\nSplitLearning,LargeLanguageModels,DataPrivacy,Federated\nvateadaptationofLLMsunderconstrainedlocalresources,various\nLearning\nmethodshavebeenproposed.Off-sitetuning[42]andknowledge\nACMReferenceFormat: distillation[13]leveragecompactlanguagemodelstoapproximate\nZixuanGu,QiufengFan,LongSun,YangLiu,andXiaojunYe.2025.VFLAIR-\nthebehavioroftargetLLMs.However,thesemethodsoftensuf-\nLLM:AComprehensiveFrameworkandBenchmarkforSplitLearningof\nferfromnotableperformancedegradationandrequirecomplex\n∗Corresponding author, also affiliated with the Shanghai Artificial Intelligence algorithmicimplementations.\nLaboratory. AnalternativeapproachisSplitLearning(SL)[10,37],acollab-\norativetrainingparadigmdevelopedbasedonFederatedLearn-\ning(FL)[22,43].Itintroducesacross-siloscenariowhereamodel\nThisworkislicensedunderaCreativeCommonsAttribution4.0InternationalLicense. is partitioned across participants, offering the benefit of minor\nKDD’25,Toronto,ON,Canada performancedegradationandasimpleyeteffectivealgorithmic\n©2025Copyrightheldbytheowner/author(s).\nimplementation. However, this solution still faces considerable\nACMISBN979-8-4007-1454-2/2025/08\nhttps://doi.org/10.1145/3711896.3737411 privacy concerns[3], as the server may attempt to infer clients’\n5470\nKDD’25,August3–7,2025,Toronto,ON,Canada ZixuanGu,QiufengFan,LongSun,YangLiu,andXiaojunYe\nlocal data through various privacy attacks[45]. Various defense forSL-LLM,demonstratingsuperiortrainingperformance.[3]pro-\nmethods[7,24]havealsobeenproposedtoaddresstheserisks. posedSplitLLM,whichpartitionsthemodelinto3parts:\"head\",\nInthiswork,wefocusonleveragingSLfortheprivateadaptation \"body\"and\"tail\"(termedHBTinthefollowingdiscussion).Itin-\nofLLMs.Aimingtosupportrelevantresearchandapplications,we troduces a novel data reconstruction attack(BiSR, tested in our\ndesign a lightweight and highly extensible Split Learning LLM followingevaluations)toinvertdatainput,highlightingthepo-\nframework,namedVFLAIR-LLM. tentialprivacyrisksinSL-LLM.Whiletheseeffortshavelaidthe\nVFLAIR-LLM incorporates basic modules for customizable SL- groundwork for SL-LLM research, they primarily focus on spe-\nLLMinferenceandfine-tuning,includinguser-definedLLMparti- cificmethodologies,withlimitedattentiongiventocomprehensive\ntion,defensestrategies,andotherrelevantfunctions.Itsupports privacybenchmarkingandthedevelopmentofuser-friendly,ex-\n3 types of LLM architect and 3 corresponding task types, each tensibletoolsforbroaderadoption.Thisgapinspiredustodevelop\nwithrelevantdatasetsavailablefordirectusage,andisopento aframeworkthatnotonlyprovidescomprehensiveprivacyalgo-\nuserstoaddnewdatasets.Toenableflexibleprivacyassessment rithmsbutalsoprioritizeseaseofuseandextensibility.\nandalgorithmdevelopment,VFLAIR-LLMalsooffersmultipleattack\nanddefensemethodsinamodularstyle,ensuringeasyusageand Data Party Model Party\nextension.Insummary,ourcontributionsarelistedbelow: Sensitive data holder with limited computation resources LLM provider with abundant computation resources\n• WedevelopalightweightframeworknamedVFLAIR-LLM HT Model Head ℳ𝒉𝒆𝒂𝒅 Model Tail ℳ𝒕𝒂𝒊𝒍\nembedding + 𝑛ℎ𝑒𝑎𝑑 encoders/decoders 𝑛𝑡𝑎𝑖𝑙 encoders/decoders + head layer\nforsplitlearningofLLMs.Thisframeworkincorporatesan\neasilyadaptablemodelpartitionmethodforawidevariety Full LLM Em L b a e y d e d r ing E D n e c c o o d d e e r r / E D n e c c o o d d e e r r / … E D n e c c o o d d e e r r / E D n e c c o o d d e e r r / L H a e y a e d r\no f a e t f a t L a t L u ck M ri s n s (L . g A IA 3 d ) m d , i a o t n i d o d e n l a 9 i l n l d y v e , e f i e r t s n a i s d o e d n s r . a e t s t s a e c s k p s o (M ss I ib A l ) e , p 2 r l i a v b a e c l y i c n o fe n r c e e n rn ce s HBT M 𝑛ℎ o 𝑒 d 𝑎𝑑 e e l e m n H b c e o e d d d a e i r d n s g / d ℳ + eco 𝒉 d 𝒆 e 𝒂 rs 𝒅 M 𝑛𝑏 o 𝑜𝑑 d 𝑦 e e l n B co o d d er y s/d ℳ eco 𝒃 d 𝒐 e 𝒅 rs 𝒚 𝑛 M 𝑡𝑎𝑖𝑙 o e d n e c h o e l d a T d er a l s a / i d y l e e ℳ r cod 𝒕 e 𝒂 rs 𝒊 𝒍 +\n• We conduct a comprehensive benchmark on attacks and\nFigure1:LLMPartition\ndefenseswithintheSL-LLMsettingusingVFLAIR-LLM.The\nbenchmarkprovidesvariousrecommendationsandinsights\n3 FrameworkOverview\non model partition configuration, defense strategies, and\nrelevanthyperparameterselectiontofacilitateeasyusage. 3.1 SplitLearningforLLM(SL-LLM)\nVFLAIR-LLMproposesageneralSplitLLMframeworkwithaData\n2 RelatedWorks PartyandaModelParty.Datapartysimulatesparticipantsequipped\nwithdataandlabelsbutconstrainedcomputationalresourcesfor\n2.1 PrivateadaptationofLLMsunderlimited\ncomprehensiveLLMutilization,possessingonlyafewlayersofa\nlocalresources completeLLM.Meanwhile,theModelpartysimulatestheLLM\nVariousmethodshavebeenproposedtoaddressthechallenges provider,retainingthemajoritysegmentoftheLLM.\nofprivateadaptationofLLMsunderconstrainedlocalresources.\n3.1.1 LLMPartitioninVFLAIR-LLM. AsdescribedinFigure1,we\nOffsite-Tuning[42,44]andknowledgedistillationHsiehetal.[13]\npropose2SL-LLMsettings:\"Head-Tail\"(HT)SL-LLMand\"Head-\nfocusontrainingsmaller,task-specificmodelslocallytoemulate\nBody-Tail\"(HBT)SL-LLM,dependingonhowtheLLMmodel\nthetraditionalLLMadaptationprocess.However,theyoftenface\nispartitionedamongparties,Forencoder-onlyanddecoder-only\ntrade-offsinmodelperformanceduetotheinherentlimitationsof\nLLMs,themodelissplitatcustomizablepointswithintheencoder\nsmallermodels.AnotherpossiblesolutiontothisconcernisSplit\nordecodersequenceforbothHTandHBTsettings.Forencoder-\nLearning(SL)[11,34],anevolutionofFederatedLearning(FL)[22]\ndecoderLLMs,HBTsplitsthemodelattheencodersequenceand\nwhere the model is partitioned across collaborators. In this ap-\nthedecodersequence.WhileHTpartitionsonlytheencoderse-\nproach,participantscollaborativelytrainanLLMbyexchanging\nquence.\nmodelintermediateandgradients,allowingthedataholdertotrain\nonlyasmallportionofthefullLLM.Variousprojectsandframe- 3.1.2 Head-Tail(HT) SL-LLM. In HT SL-LLM, a full LLM with\nworks[41,50,52]havebeendevelopedtofacilitateresearchand 𝑛 = 𝑛 ℎ𝑒𝑎𝑑 +𝑛 𝑡𝑎𝑖𝑙 encoders/decoders is separated into a Model\ndeployment in this area. For example, VFLAIR [52] is an open- HeadandaModelBody[19,30].ModelHeadMℎ𝑒𝑎𝑑 isallocated\nsourced library that supports SL training with a wide range of totheDataParty,containingtheembeddinglayerand𝑛 ℎ𝑒𝑎𝑑 en-\nmodels,datasets,andprotocols. coders/decoders.ModelTailM𝑡𝑎𝑖𝑙 istherestofLLMheldbythe\nModelParty,containing𝑛 𝑡𝑎𝑖𝑙 encoders/decodersandaheadlayer.\nTypically,𝑛 𝑡𝑎𝑖𝑙 issetsignificantlylargerthan𝑛 𝑡𝑎𝑖𝑙.\n2.2 SplitLearningofLLMs\nDuringforwardpropagation,thedatapartyperformsforward\nAssummarizedinTable1,severalstudieshaveexploredSLfor propagationfirst.Intermediate𝐻 1 =𝑀 ℎ𝑒𝑎𝑑(𝑋)isthentransmitted\nfine-tuningandinferenceofLLMs.SAP[30]isaprivacy-preserving tothemodelpartyforfurthergenerationofthefinaloutput𝑌ˆ =\nfederatedfine-tuningframeworkwhereaLLMisdividedinto2 𝑀 𝑡𝑎𝑖𝑙(𝐻 1 ).Backwardpropagationisperformedinareversedorder\nparts:\"head\"and\"tail\"(termedHTinthefollowingdiscussion),aim- withthemodelpartytransmittingtheintermediate’sgradient𝐺 1\ningtodefendmodelinversionattacks.Alsoleveraginga\"head-tail\" backtothedataparty.Thedetailedtrainingalgorithmisdescribed\npartitioning,SplitLoRA[19]introducedafine-tuningframework inAlgorithm1andFigure2a.NotethatHTSL-LLMassumesthe\n5471\nVFLAIR-LLM:AComprehensiveFrameworkandBenchmarkforSplitLearningofLLMs KDD’25,August3–7,2025,Toronto,ON,Canada\nTable1:SummaryofSL-LLMframeworks.\nSL-LLMPartition Attack WorkMode EvaluationMetrics\nLLM Fine-tuning\nModel Label Defense\nTypes Head-Tail Head-Body-Tail Strategy Standalone Distributed Performance Privacy Efficiency\nInversion Inference\nSAP[30] 1 ✓ ✓ 0 ✓ ✓ ✓ ✓\nSplitLoRA[19] 1 ✓ ✓ ✓ 0 ✓ ✓ ✓ ✓\nSplitLLM[3] 11 ✓ ✓ ✓ 3 ✓ ✓ ✓\nVFLAIR-LLM 16 ✓ ✓ ✓ ✓ 9 ✓ ✓ ✓ ✓ ✓ ✓\nData Party Model Party\nmodelpartycanaccesstheinferenceresultsorlabels.Forscenarios\nwherelabelandinferenceresultsneedtobefurtherprotectedfrom Gradient 𝐺2\ntheModelparty,wefurtherintroducetheHBTSL-LLM[3]. Ba F c o k r w wa ar r d d Label 𝑌 Loss 𝐿 Final Pred 𝑌෠ Model Tail 𝑴𝒕𝒂𝒊𝒍\nData𝑋 Model Head 𝑴𝒉𝒆𝒂𝒅 Gradient 𝐺1\n𝒏𝒕𝒂𝒊𝒍encoders + head layer\n3.1.3 Head-Body-Tail(HBT)SL-LLM. TheHBTSL-LLMsplitsafull embedding + 𝒏𝒉𝒆𝒂𝒅encoders Intermediate 𝐻1\nLLMwith𝑛=𝑛 ℎ𝑒𝑎𝑑+𝑛 𝑏𝑜𝑑𝑦+𝑛 𝑡𝑎𝑖𝑙 encoders/decodersinto3parts.\nModel Head Mℎ𝑒𝑎𝑑 contains the embeddinglayer andthe first (a)HBSL-LLM\n𝑛 ℎ𝑒𝑎𝑑 encoder/decoderlayers,whichisallocatedtotheDataParty. Data Party Model Party\nModelBodyM𝑏𝑜𝑑𝑦 contains𝑛 𝑏𝑜𝑑𝑦 encoders/decoders,themain Label 𝑌 Forward\nBackward\nbodypartofLLM,andisallocatedtotheModelParty.ModelTail Loss 𝐿\nM allo 𝑡𝑎 c 𝑖 a 𝑙 te c d on to ta t in h s e 𝑛 D 𝑡 a 𝑎 t 𝑖 a 𝑙 e P n a c r o ty d . e B rs y /d a e ll c o o c d a e ti r n s g a b n o d t a h h th e e ad m l o ay d e e r l , h w ea h d ic a h n i d s Final Pred 𝑌෠ 𝒏𝒕𝒂 M 𝒊𝒍e o n d co e d l e T rs a + i l h 𝑴 ead 𝒕𝒂 la 𝒊𝒍 yer In G te r r a m d e ie d n ia t t 𝐺 e 2 𝐻2 Mod 𝒏 e 𝒃𝒐 l 𝒅 B 𝒚 o en d c y od 𝑴 ers 𝒃𝒐𝒅𝒚\nmodeltailtodataparty,thissettingcanhinderdirectlabelinference\na\nis\nn\ns\nd\ne\nm\nts\no\ni\nd\ng\ne\nn\nl\nifi\nou\nca\ntp\nn\nu\ntl\nt\ny\nin\nla\nfr\nr\ni\ng\nn\ne\ng\nr\ne\nt\nm\nh\ne\na\nn\nn\nt\n𝑛\nb\nℎ\ny\n𝑒𝑎\nth\n𝑑\ne\na\nm\nnd\nod\n𝑛\ne\n𝑡\nl\n𝑎\np\n𝑖𝑙\na\n.\nrty.Typically,𝑛 𝑏𝑜𝑑𝑦 Data𝑋 em M be o d d di e n l g H + e 𝒏 a 𝒉𝒆 d 𝒂 𝒅 𝑴 en 𝒉 c 𝒆 o 𝒂 d 𝒅 ers In G te r r a m d e ie d n ia t t 𝐺 e 1 𝐻1\nDuringtheforwardprocess,thedatapartyfirstfeedsinputdata (b)HBTSL-LLM\nintoitsmodelhead.Itsintermediate𝐻\n1\n=𝑀 ℎ𝑒𝑎𝑑(𝑋)isthentrans-\nFigure2:TrainingProcessofSL-LLM\nmitted to the model party for model body forward calculation:\n𝐻 2 = 𝑀 𝑏𝑜𝑑𝑦(𝐻 1 ).Finally,themodelbodyoutputistransmitted 2SL-LLMPartitionSettings. VFLAIR-LLMofferstwoLLMpar-\nbacktodatapartytogeneratefinalpredictions𝑌ˆ usingthemodel titionsettings:Head-Body(HT)SL-LLMandHead-Body-Tail(HBT)\ntail.Duringthebackwardpropagation,datapartyandmodelparty SL-LLMasdescribedinSection3.1.\nconsecutivelycalculategradients𝐺 1and𝐺 2foritsreceivedinterme-\n2UsagePipelines. VFLAIR-LLMsupportsbothLLMfine-tuning\ndiatesandperformlocalbackwardcalculations.Detailedtraining\nand LLM inference. In Inference pipeline, users can load a pre-\nalgorithmisdescribedinAlgorithm2andFigure2b.\ntrained LLM to conduct direct inference on a given dataset. In\nFine-tunepipeline,userscanfine-tuneanLLMonadownstream\n3.2 Fine-tuningMethodsforSL-LLM task.\nToenableefficientLLMfine-tuning,variousfine-tuningstrategies[14,\n16LLMTypes. Currently,wesupport16LLMsasshowninTa-\n19]havebeenproposed.VFLAIR-LLMenablesuserstocustomize\nble2.Toenableeasyextensionandcompatibility,allmodelsplitsare\ntheirownfine-tuningstrategies,includingFull-Tuning,whereall\nimplementedbasedontheTransformers[36]librarywithdetailed\nmodelparametersaretrainable,andLocal-Tuning,whereonly\nguidanceinourcodebase.\nthedataparty’ssub-modelistrainable.Afterspecifyingthetrain-\nablemodelsegments,wealsoincorporatethePEFTLibrary[21] Table2:SupportedLLMTypes\nintoVFLAIR-LLM,enablingsupportforawiderangeofparameter-\nefficientfine-tuning(PEFT)methods,includingLoRA[14],LoKr[15], Structure LLMTypes\nAdaLoRA[48],andLoHa[15]etc.Partiescanchoosetoapplyeither Encoder-only BertRobertaAlbert\naVanillafine-tuningstrategyoraLoRAstrategytotheirown GPT2LlamaBaichuan2ChatGLM2FalconGemma\nDecoder-only\nMambaMistralQwen2DeepseekMiniCPMQwen2-VL\nmodelsegments.\nEncoder-DecoderQwen T5\n3.3 VFLAIR-LLMFrameworkDesign\n3BasicLLMArchitects. VFLAIR-LLMsupport3commonlyused\nBasedonthecodebaseof VFLAIR[52],ageneralframeworkfor\nLLMarchitectsaspresentedinTable3,eachfeaturingadifferent\nverticalfederatedlearning,wedevelopVFLAIR-LLM,aframework\nheadlayeraddedtothemainLLMbodytosuitdownstreamtasks.\nspecificforimplementingandbenchmarkingSL-LLMscenarios,as\nillustratedinFigure3.VFLAIR-LLMsharesVFLAIR’sconfiguration 2WorkModes. VFLAIR-LLMsupport2workmodes:standalone\ndesign,partyloadingmodule,andbasiccommunicationfunctions, simulationanddistributeddeployment,supportingbothsimulation\nbutfocusesonLLM-centereddatasetsandtasks,fine-tuningstrate- researchandreal-worldapplications.Weprovideanefficiencycom-\ngies,andattackanddefenseevaluations. parisonbetweendistributedandstandalonemodeinAppendixC.\n5472\nKDD’25,August3–7,2025,Toronto,ON,Canada ZixuanGu,QiufengFan,LongSun"
  }
}