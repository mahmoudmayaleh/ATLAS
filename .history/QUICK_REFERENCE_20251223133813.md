# ATLAS PROJECT: QUICK REFERENCE SUMMARY

## Paper-by-Paper Overview

### 1. ATLAS_Base_Specification (1 page)

**Status:** Project charter and objectives  
**Key Content:** Problem definition, three-phase plan, success criteria  
**Critical Points:**

- Gap: MIRA has task awareness but centralized; HSplitLoRA heterogeneous but task-blind
- Solution: Combine gradient clustering + split learning + adapter aggregation
- Three phases: Analysis → Simulation → Evaluation

---

### 2. ATLAS_V1 (22 pages)

**Status:** Detailed project plan with presentations  
**Key Content:** Challenges, system design, 4-phase architecture, 12-week timeline  
**Critical Innovations:**

- **Phase 1:** 64-D task fingerprint + k-Means clustering
- **Phase 2:** Weight importance scoring + heterogeneous LoRA ranks
- **Phase 3:** Split learning with LoRA adapters
- **Phase 4:** Noise-free heterogeneous aggregation

**Expected Results:**

- Accuracy: +20-30% improvement
- Memory: 30-40% reduction
- Communication: Minimal overhead

---

### 3. MIRA (5 pages)

**Key Concept:** Federated Multi-Task Learning with graph-based regularization

**Objective Function:**
$$J(W) = F(W) + \lambda R(W)$$

- F(W): Local training losses
- R(W): Laplacian regularization for task similarity

**Main Algorithm:**

1. Local update: Client trains LoRA adapters locally
2. Server update: Aligns models of similar tasks
   $$\Delta W_k = \Delta W_{k,R} - \eta\lambda \sum_{\ell \in N_k} a_{k\ell}(\Delta W_{k,R} - \Delta W_{\ell,R})$$

**Why it matters for ATLAS:** Task clustering mechanism

---

### 4. HSplitLoRA (16 pages)

**Key Concept:** Heterogeneous parameter-efficient split learning

**Three Core Innovations:**

1. **Important Weight Identification:** Rank weights by contribution
2. **Dynamic Rank Configuration:** Assign ranks per device budget
3. **Noise-Free Aggregation:** Concatenate + merge low-rank matrices

**Aggregation Innovation:**

```
Client 1: rank 4 matrices [B₁, A₁]
Client 2: rank 6 matrices [B₂, A₂]
           ↓ Concatenate ↓
Merged: [B₁|B₂] + [A₁|A₂] (rank 10)
           ↓ Weighted Average ↓
Decompose back to original ranks (via SVD)
```

**Why it matters for ATLAS:** Heterogeneous adaptation + aggregation

---

### 5. SplitLoRA (9 pages)

**Key Concept:** First open-source SL-LLM framework combining SFL + LoRA

**Architecture:**

- Client: Early layers + LoRA adapters
- Server: Deep layers + LoRA adapters
- Exchange: Intermediate activations + gradients

**Performance (GPT2-M):**

- BLEU: 70.09-70.26 (excellent)
- Memory: 0.008M-0.062M (73-87% reduction)
- Communication: 100× less than full parameter transfer

**Why it matters for ATLAS:** Training loop foundation

---

### 6. Privacy-Aware SFL (12 pages)

**Key Concept:** Multi-objective optimization balancing privacy, efficiency, convergence

**Three Objectives to Optimize:**

1. Minimize: Privacy leakage (Ψ_u) based on Fisher information
2. Minimize: Training time (T_total)
3. Minimize: Energy consumption (E_total)

**Optimization Variables:**

- Split layer l_u per device
- Transmit power P_u per device
- Bandwidth allocation B_u per device

**Algorithm:** ε-constraint BCD

1. Bandwidth: Monotonicity analysis
2. Power: Bisection search
3. Split layer: Successive Convex Approximation

**Results:** 24% faster convergence, 40% lower energy, 7% reduced privacy leakage

**Why it matters for ATLAS:** Configuration optimization strategy

---

### 7. VFLAIR-LLM (12 pages)

**Key Concept:** Comprehensive privacy attack/defense benchmark for SL-LLM

**Framework Features:**

- 16 LLM types supported
- 18 datasets across 3 task types
- 5 attacks × 9 defenses evaluated

**Attacks:**

1. VMI: Vanilla model inversion
2. RMI: Recursive model inversion
3. BiSR: Bidirectional semantic reconstruction (novel)
4. LIA: Label inference attacks

**Defenses:**

1. DP (Differential Privacy)
2. MID (Mutual Information Disentanglement) - BEST
3. AT (Adversarial Training)
4. SP (Sparsification)
5. SnD (Split-N-Denoise)
6. Text-level: SanText, CusText, RanText
7. TO (Token Obfuscation)

**Privacy Metric:**
$$\text{DCS} = \alpha × \text{MP} + (1-\alpha) × (1-\text{AP})$$

- MP: Main task performance
- AP: Attack performance (lower is better)
- Target: DCS ≥ 0.7 for good privacy-utility balance

**Why it matters for ATLAS:** Privacy evaluation framework

---

## ATLAS INTEGRATION ROADMAP

### Component Mapping

```
MIRA → Task Clustering (Phase 1)
  - Gradient fingerprinting: 64-D vectors
  - k-Means clustering
  - Laplacian graph construction

HSplitLoRA → Heterogeneous Adaptation (Phase 2) + Aggregation (Phase 4)
  - Weight importance scoring
  - Dynamic rank assignment
  - Noise-free concatenation + merge

SplitLoRA → Training Loop (Phase 3)
  - Client-server split architecture
  - LoRA adapter management
  - Intermediate activation exchange

Privacy-Aware SFL → Configuration Optimization
  - Multi-objective optimization
  - Split layer, power, bandwidth joint optimization

VFLAIR-LLM → Privacy Evaluation (Phase 5)
  - Attack implementation
  - Defense application
  - DCS computation
```

### Implementation Checklist

**Phase 1: Task Clustering**

- [ ] Compute gradient-based task embeddings
- [ ] Implement k-Means clustering
- [ ] Validate cluster quality (silhouette score)
- [ ] Construct task similarity graph

**Phase 2: Configuration Assignment**

- [ ] Implement weight importance scoring
- [ ] Design dynamic rank selection algorithm
- [ ] Select split points per device
- [ ] Validate configurations against memory constraints

**Phase 3: Training Loop**

- [ ] Implement client-side forward propagation
- [ ] Implement server-side forward/backward propagation
- [ ] Exchange intermediate activations
- [ ] Update LoRA adapters using SGD

**Phase 4: Aggregation**

- [ ] Implement noise-free concatenation
- [ ] Implement weighted merging
- [ ] Decompose back to original ranks
- [ ] Broadcast aggregated adapters

**Phase 5: Privacy & Evaluation**

- [ ] Integrate VFLAIR-LLM library
- [ ] Implement/test key attacks
- [ ] Implement/test key defenses
- [ ] Compute accuracy, efficiency, privacy metrics

---

## KEY TECHNICAL DECISIONS

### 1. Task Fingerprint Dimensionality

**Options:** 32-D, 64-D, 128-D  
**Recommendation:** Start with 64-D (from ATLAS V1 plan)  
**Validation:** Silhouette analysis, cluster stability

### 2. Clustering Algorithm

**Options:** k-Means, DBSCAN, Hierarchical  
**Recommendation:** k-Means (simple, proven in MIRA, ATLAS V1)  
**Tuning:** k value selection (3, 5, 10)

### 3. Aggregation Strategy

**Options:** Direct averaging, concatenation+merge, federated averaging  
**Recommendation:** Concatenation+merge (HSplitLoRA proven method)  
**Advantage:** No information loss

### 4. Privacy Defense Strategy

**Options:** DP, MID, AT, combinations  
**Recommendation:** MID as primary (best DCS from VFLAIR-LLM)  
**Fallback:** DP for provable privacy guarantees

### 5. Model Split Point

**Options:** After layer 1, 2, 4, 8, 12 (for 12-layer BERT/GPT2)  
**Recommendation:** Device-adaptive (HSplitLoRA approach)  
**Constraint:** Respect memory budget

---

## EXPECTED PERFORMANCE TARGETS

| Metric            | ATLAS Target           | Reference        |
| ----------------- | ---------------------- | ---------------- |
| **Accuracy**      | +15-25% vs HSplitLoRA  | ATLAS V1         |
| **Memory**        | 30-40% reduction       | ATLAS V1         |
| **Communication** | SplitLoRA baseline     | SplitLoRA        |
| **Privacy (DCS)** | ≥ 0.7                  | VFLAIR-LLM       |
| **Convergence**   | Faster than task-blind | MIRA, HSplitLoRA |

---

## IMPLEMENTATION TIMELINE

| Weeks | Task                       | Deliverable           |
| ----- | -------------------------- | --------------------- |
| 1-2   | Setup, SplitLoRA loop      | Training framework    |
| 3-4   | Task clustering            | Phase 1 complete      |
| 5-6   | Weight importance + ranks  | Phase 2 complete      |
| 7-8   | Aggregation                | Phase 4 complete      |
| 9-10  | Integration + optimization | Full pipeline         |
| 11-12 | Privacy + evaluation       | Comprehensive results |

---

## OPEN QUESTIONS

1. **Task Fingerprint Quality:** How well do 64-D gradient vectors capture task similarity?

   - Validate via: Cluster quality metrics, downstream accuracy impact

2. **Convergence Guarantees:** Does task clustering slow down global convergence?

   - Analyze: Convergence curves, inter-cluster communication needs

3. **Optimal k (num clusters):** How many clusters for different dataset sizes?

   - Experiment: k=3, 5, 10, estimate via elbow method

4. **Privacy Defenses:** Which defense strategy (single vs. multi) is best?

   - Test: MID alone vs. MID+DP combinations using VFLAIR-LLM

5. **Scalability:** How many clients/devices can ATLAS support?
   - Benchmark: 10, 50, 100, 1000 client scenarios

---

## REFERENCES

**Primary Papers:**

1. ATLAS Base Specification (2025/2026, Project #13)
2. ATLAS V1 (December 2025 Plan)
3. MIRA (IEEE Networking Letters, Vol. 7, Sep 2025)
4. HSplitLoRA (arXiv:2405.07520, May 2024)
5. SplitLoRA (arXiv:2407.02590, July 2024)
6. Privacy-Aware SFL (IEEE IoT Journal, DOI 10.1109/JIOT.2025.3600269)
7. VFLAIR-LLM (KDD'25, August 2025)

**Open-Source Frameworks:**

- VFLAIR-LLM: https://github.com/FLAIR-THU/VFLAIR-LLM
- SplitLoRA: https://fdu-inc.github.io/splitlora/
- PEFT: https://github.com/huggingface/peft
- Transformers: https://github.com/huggingface/transformers

---

## CRITICAL SUCCESS FACTORS

1. **Task Clustering Quality:** Good fingerprints → good clusters → better knowledge sharing
2. **Heterogeneous Aggregation:** Noise-free aggregation → information preservation → better convergence
3. **Privacy-Utility Balance:** Strong defenses without excessive accuracy loss
4. **Communication Efficiency:** Minimize intermediate transmission without degrading accuracy
5. **Device Heterogeneity Support:** Truly adaptive configurations per device budget

---

## RISK MITIGATION

| Risk                         | Mitigation                                                      |
| ---------------------------- | --------------------------------------------------------------- |
| Task fingerprints too coarse | Use finer resolution (128-D) or different fingerprinting method |
| Cluster instability          | Use k-Means++ init, lock clusters after discovery               |
| Slow convergence             | Add inter-cluster communication or increase cluster size        |
| Privacy vulnerabilities      | Combine defenses (MID+DP), use VFLAIR-LLM to validate           |
| Communication bottleneck     | Implement sparsification, quantization, compression             |
| GPU memory exceeded          | Reduce model size, increase split depth, reduce batch size      |
