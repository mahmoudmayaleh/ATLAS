# ATLAS: Adaptive Task-aware Federated Learning with Heterogeneous Splitting

**Master's Advanced Project** | **Timeline:** 12 weeks | **Status:** Phase 2 Complete âœ…

---

## ğŸ¯ Project Overview

ATLAS combines three cutting-edge techniques for privacy-preserving federated learning with Large Language Models:

1. **MIRA** - Task-aware client clustering using gradient fingerprints
2. **HSpLitLoRA** - Heterogeneous LoRA ranks adapted to device capabilities
3. **SplitLoRA** - Split learning for 10-100x communication reduction

**Key Innovation:** Privacy-aware aggregation without noise, maintaining accuracy while protecting client data.

---

## ğŸ“ Project Structure

```
ATLAS/
â”œâ”€â”€ src/                              # Source code
â”‚   â”œâ”€â”€ phase1_clustering.py          # âœ… Task clustering (MIRA)
â”‚   â””â”€â”€ phase2_configuration.py       # âœ… Heterogeneous config (HSplitLoRA)
â”œâ”€â”€ tests/                            # Unit tests
â”‚   â”œâ”€â”€ test_phase1.py                # âœ… 16 tests passing
â”‚   â””â”€â”€ test_phase2.py                # âœ… 20 tests passing
â”œâ”€â”€ phase1_demo.ipynb                 # âœ… Interactive demo
â”œâ”€â”€ requirements.txt                  # Dependencies
â”œâ”€â”€ ATLAS_IMPLEMENTATION_ROADMAP.md   # Detailed technical specs
â”œâ”€â”€ ATLAS_REVISED_PLAN_SUMMARY.md     # Executive summary
â””â”€â”€ README.md                         # This file
```

---

## ğŸš€ Quick Start

### 1. Install Dependencies

```powershell
cd c:\Users\Hp\Downloads\Advanced_project\ATLAS
pip install -r requirements.txt
```

### 2. Run Tests

```powershell
# Phase 1 tests (16 tests)
python tests\test_phase1.py

# Phase 2 tests (20 tests)
python tests\test_phase2.py
```

### 3. Run Demo

Open `phase1_demo.ipynb` in VS Code or Jupyter:

```powershell
jupyter notebook phase1_demo.ipynb
```

---

## âœ… Implementation Status

### Phase 1: Task Clustering (Weeks 1-2) âœ… COMPLETE

**Components:**

- âœ… `GradientExtractor` - Extract 64-D fingerprints via PCA
- âœ… `TaskClusterer` - k-Means with auto k-selection
- âœ… `visualize_clusters()` - t-SNE/PCA visualization

**Tests:** 16/16 passing âœ…

**Demo:** [phase1_demo.ipynb](phase1_demo.ipynb)

**Usage:**

```python
from src.phase1_clustering import GradientExtractor, TaskClusterer

# Extract fingerprints
extractor = GradientExtractor(dim=64, device='cpu')
extractor.fit(gradient_list)
fingerprints = extractor.extract_batch(gradient_list)

# Cluster clients
clusterer = TaskClusterer(n_clusters_range=(2, 5))
result = clusterer.cluster(fingerprints)

# Get task groups
task_groups = clusterer.get_task_groups(client_ids)
```

---

### Phase 2: Heterogeneous Configuration (Weeks 2-3) âœ… COMPLETE

**Components:**

- âœ… `DeviceProfiler` - Profile device capabilities (CPU/edge GPU/GPU)
- âœ… `WeightImportanceScorer` - Compute parameter importance
- âœ… `RankAllocator` - Allocate heterogeneous LoRA ranks

**Tests:** 20/20 passing âœ…

**Usage:**

```python
from src.phase2_configuration import DeviceProfiler, WeightImportanceScorer, RankAllocator

# Profile device
profiler = DeviceProfiler()
device_profile = profiler.profile_device('gpu')

# Compute importance
scorer = WeightImportanceScorer()
importance = scorer.compute_importance(gradients)

# Allocate ranks
allocator = RankAllocator(model_dim=768)
ranks = allocator.allocate_ranks(device_profile, importance, n_layers=12)

# Validate memory constraint
is_valid, memory_mb = allocator.validate_memory_constraint(ranks, device_profile)
```

---

### Phase 3: Split Federated Learning (Weeks 3-6) âœ… COMPLETE

**Components:**

- âœ… `LoRAAdapter` - Low-rank adaptation module with configurable rank
- âœ… `SplitClient` - Client-side training with frozen base models
- âœ… `SplitServer` - Server-side computation and task-specific heads
- âœ… Communication protocol for federated training rounds
- âœ… Integration with GPT-2, LLaMA, BERT models

**Tests:** 29/29 passing âœ…

**Usage:**

```python
from src.phase3_split_fl import SplitClient, SplitServer, LoRAAdapter

# Create client with LoRA
client = SplitClient(
    client_id=0,
    model_name='gpt2',
    rank_config={'layer_0': 16, 'layer_1': 16},
    split_layer=6
)

# Create server
server = SplitServer(
    model_name='gpt2',
    n_tasks=4,
    split_layer=6,
    num_classes=2
)

# Federated training round
from src.phase3_split_fl import federated_training_round
metrics = federated_training_round(clients, server, optimizer, data_loader)
```

---

### Phase 4: Privacy-Aware Aggregation (Weeks 6-8) âœ… COMPLETE

**Components:**

- âœ… `AggregationEngine` - SVD-based heterogeneous weight merging
- âœ… `PrivacyVerifier` - Basic privacy metrics and verification
- âœ… Task-aware weighting for multi-group aggregation
- âœ… Quality metrics (reconstruction error, rank compression)

**Tests:** 27/27 passing âœ…

**Usage:**

```python
from src.phase4_aggregation import AggregationEngine, PrivacyVerifier

# Create aggregation engine
engine = AggregationEngine(target_rank=32, aggregation_method='svd')

# Aggregate client updates by task groups
aggregated = engine.aggregate_all_groups(client_updates, task_groups)

# Merge with task-aware weights
from src.phase4_aggregation import compute_task_aware_weights
weights = compute_task_aware_weights(task_groups)
global_weights = engine.weighted_merge(aggregated, weights)

# Verify privacy properties
verifier = PrivacyVerifier()
privacy_score = verifier.compute_privacy_score(task_group_size=10, n_total_clients=50)
```

---

### Phase 5: Experimental Evaluation (Weeks 8-10) ğŸ”„ NEXT

**Components to implement:**

- [ ] `FederatedTrainer` - Multi-round federated training pipeline
- [ ] `GLUEDataLoader` - Dataset integration for GLUE tasks
- [ ] `MetricsCollector` - Performance and efficiency metrics
- [ ] Dataset distribution (IID and non-IID)
- [ ] Convergence monitoring and visualization

---

### Phase 6: Benchmarking & Results (Weeks 10-12) ğŸ”„ PENDING

**Deliverables:**

- [ ] Benchmarks on GLUE, SQuAD, E2E datasets
- [ ] Comparison with baselines (Standard FL, Homogeneous LoRA)
- [ ] Performance visualization and plots
- [ ] Live demonstration script
- [ ] Final results report and tables

---

## ğŸ“Š Expected Results

| Metric                | Target                          |
| --------------------- | ------------------------------- |
| Accuracy Improvement  | +15-25% vs homogeneous baseline |
| Memory Reduction      | 30-40% on constrained devices   |
| Communication Savings | 10-100x vs standard FL          |
| Convergence Time      | < 100 rounds                    |

---

## ğŸ”§ Hardware Requirements

**Phases 1-4 (Complete):**

- âœ… CPU-only (16GB RAM sufficient)
- âœ… No GPU required
- âœ… ~100MB memory for 50 clients

**Future (Phase 3+):**

- ğŸ”„ GPU recommended (but CPU-compatible)
- ğŸ”„ 100+ GPU-hours for full experiments
- ğŸ”„ 8GB+ VRAM for LLaMA-7B

---

## ğŸ“š Key Documents

- **[ATLAS_IMPLEMENTATION_ROADMAP.md](ATLAS_IMPLEMENTATION_ROADMAP.md)** - Detailed technical specifications for all 6 phases
- **[ATLAS_REVISED_PLAN_SUMMARY.md](ATLAS_REVISED_PLAN_SUMMARY.md)** - Executive summary with research background
- **[QUICK_REFERENCE_CARD.md](QUICK_REFERENCE_CARD.md)** - Quick reference for milestones and metrics
- **[COMPREHENSIVE_RESEARCH_ANALYSIS.md](COMPREHENSIVE_RESEARCH_ANALYSIS.md)** - Deep dive into research papers

---

## ğŸ§ª Testing

All tests run on CPU without GPU requirement.

```powershell
# Run all tests
python tests\test_phase1.py && python tests\test_phase2.py

# Expected output:
# Phase 1: 16/16 tests passing âœ…
# Phase 2: 20/20 tests passing âœ…
```

**Test Coverage:**

- Unit tests for all components
- Integration tests for end-to-end workflows
- Memory constraint validation
- Importance score normalization checks

---

## ğŸ“ Academic Context

**Problem:** Existing federated learning approaches either:

- Use homogeneous configurations (ignoring device constraints)
- Add noise for privacy (reducing accuracy)
- Require high communication bandwidth

**Our Solution (ATLAS):**

- **Task-aware clustering** improves aggregation quality (+15-25% accuracy)
- **Heterogeneous LoRA ranks** save 30-40% memory on edge devices
- **Split learning** reduces communication by 10-100x
- **Noise-free aggregation** maintains accuracy while protecting privacy

**Novelty:** First system combining MIRA + HSpLitLoRA + SplitLoRA for LLMs.

---

## ğŸ“ˆ Development Timeline

| Week  | Phase           | Status      | Deliverables                                                 |
| ----- | --------------- | ----------- | ------------------------------------------------------------ |
| 1-2   | Task Clustering | âœ… Complete | GradientExtractor, TaskClusterer, tests, demo                |
| 2-3   | Configuration   | âœ… Complete | DeviceProfiler, WeightImportanceScorer, RankAllocator, tests |
| 3-6   | Split FL        | ğŸ”„ Pending  | SplitClient, SplitServer, LoRAAdapter, training loop         |
| 6-8   | Aggregation     | ğŸ”„ Pending  | AggregationEngine, privacy tests                             |
| 8-10  | Privacy Eval    | ğŸ”„ Pending  | 5 attacks, 9 defenses, VFLAIR integration                    |
| 10-12 | Demo            | ğŸ”„ Pending  | Benchmarks, final demo, report                               |

**Current Status:** Week 2-3 âœ…

---

## ğŸ› Troubleshooting

### Import Errors

```python
# Add to path if needed
import sys
sys.path.insert(0, 'c:/Users/Hp/Downloads/Advanced_project/ATLAS/src')
```

### PyTorch Installation

```powershell
# CPU-only version
pip install torch --index-url https://download.pytorch.org/whl/cpu
```

### Memory Issues

- Reduce `n_clients` in examples (default: 50, can use 10-20)
- Reduce `dim` in GradientExtractor (default: 64, can use 32)
- Use smaller model dimensions

---

## ğŸ¤ Contributing

This is a Master's thesis project. For questions or suggestions:

1. Review [ATLAS_IMPLEMENTATION_ROADMAP.md](ATLAS_IMPLEMENTATION_ROADMAP.md)
2. Check existing tests for usage examples
3. Run tests to verify changes

---

## ğŸ“ Citation

```bibtex
@mastersthesis{atlas2025,
  title={ATLAS: Adaptive Task-aware Federated Learning with Heterogeneous Splitting},
  author={[Your Name]},
  year={2025},
  school={[Your University]},
  note={Combines MIRA, HSpLitLoRA, and SplitLoRA for privacy-preserving federated learning with LLMs}
}
```

---

## ğŸ“„ License

[To be determined - typically MIT or Apache 2.0 for research code]

---

## ğŸ¯ Next Steps

**For continued development:**

1. **Phase 3 (Weeks 3-6):**

   ```powershell
   # Review implementation specs
   code ATLAS_IMPLEMENTATION_ROADMAP.md

   # Start implementing SplitClient
   # See Phase 3 section for detailed code structure
   ```

2. **Get GPU Access:**

   - Phase 3+ requires GPU for efficient training
   - Can still develop/test on CPU with smaller models
   - Consider cloud GPU (Google Colab, AWS, Azure)

3. **Prepare Datasets:**
   ```powershell
   # Download GLUE benchmark
   pip install datasets
   python -c "from datasets import load_dataset; load_dataset('glue', 'sst2')"
   ```

---

**Last Updated:** December 24, 2025  
**Version:** 0.2.0  
**Status:** Phase 1 & 2 Complete âœ… | Phase 3-6 Pending ğŸ”„
