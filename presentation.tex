\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{shapes, arrows, positioning, fit, calc, matrix, decorations.pathreplacing}

% Color scheme
\definecolor{darkblue}{RGB}{0, 51, 102}
\definecolor{lightblue}{RGB}{135, 206, 250}
\definecolor{darkgreen}{RGB}{34, 102, 68}
\definecolor{orange}{RGB}{255, 140, 0}
\definecolor{purple}{RGB}{128, 0, 128}
\definecolor{red}{RGB}{200, 0, 0}

\usetheme{Madrid}
\usecolortheme{default}
\setbeamercolor{primary}{bg=darkblue, fg=white}
\setbeamercolor{secondary}{bg=lightblue, fg=darkblue}

\title{ATLAS: Adaptive Task-aware Federated Learning with LoRA-based Heterogeneous Splitting}
\subtitle{Supervisor Meeting - Midterm Progress Update}
\author{Advanced Master's Project}
\date{January 7, 2026}

\begin{document}

% ============================================================================
% TITLE SLIDE
% ============================================================================
\begin{frame}
  \titlepage
  \note{Opening slide - introduce the project pivot and new direction}
\end{frame}


% ============================================================================
% SECTION 1: PROJECT OVERVIEW & MOTIVATION
% ============================================================================
\section{Project Overview \& Motivation}

\begin{frame}{ATLAS: System Overview}
  \textbf{Adaptive Task-aware Federated Learning with LoRA-based Heterogeneous Splitting}
  
  \vspace{0.5cm}
  
  \textbf{Core Innovation:}
  \begin{itemize}
    \item Federated fine-tuning of Large Language Models
    \item Task-aware clustering for heterogeneous workloads
    \item Personalized models via graph-based regularization
    \item Memory-efficient split learning with LoRA adapters
  \end{itemize}
  
  \vspace{0.5cm}
  
  \textbf{Key Components:}
  \begin{enumerate}
    \item Task clustering via gradient similarity
    \item Heterogeneous LoRA rank allocation
    \item Split federated learning architecture
    \item MIRA Laplacian regularization
  \end{enumerate}
\end{frame}

\begin{frame}{Research Gap \& Contribution}
  \textbf{Challenge:} Federated learning for LLMs on heterogeneous edge devices
  
  \vspace{0.3cm}
  
  \textbf{Current Limitations:}
  \begin{enumerate}
    \item Single-task FL cannot handle diverse client tasks
    \item Fixed LoRA ranks waste memory on constrained devices
    \item High communication costs limit scalability
    \item Global averaging loses task-specific specialization
  \end{enumerate}
  
  \vspace{0.3cm}
  
  \textbf{ATLAS Solution:}
  \begin{itemize}
    \item \textcolor{darkgreen}{\textbf{Task-aware clustering}} groups similar clients
    \item \textcolor{darkgreen}{\textbf{Heterogeneous LoRA}} adapts to device capabilities
    \item \textcolor{darkgreen}{\textbf{Split learning}} reduces communication by 10-100×
    \item \textcolor{darkgreen}{\textbf{Graph regularization}} preserves personalization
  \end{itemize}
\end{frame}

% ============================================================================
% SECTION 2: SYSTEM ARCHITECTURE
% ============================================================================
\section{System Architecture}

\begin{frame}{Phase 1: Task Clustering via Gradient Similarity}
  \textbf{The Challenge:} Clients have different tasks (sentiment vs medical vs QA)
  
  \textbf{Why This Matters:} Mixing incompatible tasks $\rightarrow$ worse accuracy for everyone
  
  \vspace{0.2cm}
  
  \begin{center}
  \begin{tikzpicture}[scale=0.9]
    % Clients
    \foreach \i/\x/\y/\c in {0/0/3/blue, 1/1.5/3.5/blue, 2/0.5/2/blue, 3/5.5/3/red, 4/6.5/2.5/red, 5/6/3.8/red} {
      \node[circle, draw, fill=\c!30, minimum size=1cm, line width=1.2pt, font=\small\bfseries] (c\i) at (\x,\y) {C\i};
    }
    
    % Clustering ellipses
    \draw[dashed, blue, line width=2pt] (0.5,2.7) ellipse (1.9cm and 1.8cm);
    \draw[dashed, red, line width=2pt] (6,3.1) ellipse (1.5cm and 1.5cm);
    
    % Labels
    \node[below, font=\bfseries\large] at (0.5,0.5) {\textcolor{blue}{Group 0: Sentiment}};
    \node[below, font=\bfseries\large] at (6,0.5) {\textcolor{red}{Group 1: QA}};
    
    % Task graph edges
    \draw[blue, line width=1.5pt] (c0) -- (c1);
    \draw[blue, line width=1.5pt] (c1) -- (c2);
    \draw[blue, line width=1.5pt] (c2) -- (c0);
    \draw[red, line width=1.5pt] (c3) -- (c4);
    \draw[red, line width=1.5pt] (c4) -- (c5);
    \draw[red, line width=1.5pt] (c5) -- (c3);
  \end{tikzpicture}
  \end{center}
  
  \vspace{0.2cm}
  
  \textbf{Our Solution:} Group by gradient patterns (privacy-preserving!)
  \begin{itemize}
    \item Extract 64-D fingerprint from last-layer gradients
    \item Cluster with multi-metric k-Means (stable across rounds)
    \item Aggregate only within task groups
  \end{itemize}
\end{frame}

\begin{frame}{Phase 1: Technical Deep Dive}
  \textbf{Challenge:} How to cluster without seeing data?
  
  \vspace{0.3cm}
  
  \textbf{Step 1: Gradient Extraction}
  \begin{itemize}
    \item Client does 2-3 forward/backward passes locally
    \item Extract gradients from \textit{last 2 transformer layers only}
    \item \textbf{Why?} Last layers = task-specific features, early layers = generic
    \item Normalize each layer to unit length (prevents large layer domination)
    \item Apply PCA: 5M values $\rightarrow$ 64-D fingerprint
  \end{itemize}
  
  \vspace{0.2cm}
  
  \textbf{Step 2: Multi-Metric Clustering}
  \begin{itemize}
    \item Try k=2,3,4,5 clusters
    \item Score with: 50\% Silhouette + 30\% Davies-Bouldin + 20\% Calinski-Harabasz
    \item \textbf{Why multiple metrics?} Balance separation vs compactness
    \item \textbf{Result:} Stable clusters across 10+ federated rounds
  \end{itemize}
    
  \textbf{Output for Phase 2:} Cluster variance, size, mean fingerprint
\end{frame}

\begin{frame}{Phase 2: Heterogeneous LoRA Configuration}
  \textbf{The Challenge:} Clients have vastly different device capabilities
  
  \textbf{Why Fixed Ranks Fail:}
  \begin{itemize}
    \item rank=64 for all $\rightarrow$ 2GB CPU runs out of memory (OOM crash)
    \item rank=4 for all $\rightarrow$ 16GB GPU wastes available memory
  \end{itemize}
  
  \vspace{0.2cm}
  
  \begin{center}
  \begin{tikzpicture}[scale=0.8]
    % Device capabilities
    \node[draw, rectangle, fill=red!20, minimum width=2.5cm, minimum height=1.2cm] (low) at (0,0) {
      \begin{tabular}{c}
        \textbf{Low} \\
        \tiny 2GB RAM \\
        \tiny CPU
      \end{tabular}
    };
    \node[draw, rectangle, fill=orange!20, minimum width=2.5cm, minimum height=1.2cm] (med) at (3.5,0) {
      \begin{tabular}{c}
        \textbf{Medium} \\
        \tiny 4GB RAM \\
        \tiny Edge GPU
      \end{tabular}
    };
    \node[draw, rectangle, fill=green!20, minimum width=2.5cm, minimum height=1.2cm] (high) at (7,0) {
      \begin{tabular}{c}
        \textbf{High} \\
        \tiny 8GB+ RAM \\
        \tiny GPU
      \end{tabular}
    };
    
    % Arrows to ranks
    \draw[->, thick] (low) -- ++(0,-1.5) node[below] {\small rank=4,8};
    \draw[->, thick] (med) -- ++(0,-1.5) node[below] {\small rank=8,16};
    \draw[->, thick] (high) -- ++(0,-1.5) node[below] {\small rank=16,32};
  \end{tikzpicture}
  \end{center}
  
  \vspace{0.3cm}
  
  \textbf{HSplitLoRA Formulation:}
  \begin{itemize}
    \item \textbf{Memory constraint:} $\sum_{\ell=1}^{L} 2 \cdot d \cdot r_\ell \cdot b \leq C_{mem}$
    \item \textbf{Budget allocation:} $C_{mem} = M_{device} \cdot (1 - \alpha_{base} - \alpha_{act} - \alpha_{opt})$
    \item \textbf{Importance-based:} More critical layers get higher ranks
    \item Example: CPU with 2GB → 512MB for adapters (25\% of total)
  \end{itemize}
\end{frame}

\begin{frame}{Phase 2: Greedy Rank Allocation Algorithm}
  \textbf{Given:} Memory budget $C_{mem}$, layer importance scores
  
  \textbf{Goal:} Maximize model capacity within budget
  
  \vspace{0.3cm}
  
  \textbf{Algorithm:}
  \begin{enumerate}
    \item Sort layers by importance (descending): [layer\_5, layer\_4, ...]
    \item For each layer in order:
    \begin{itemize}
      \item Try ranks: 4, 8, 16, 32, 64
      \item Compute memory: $M = 2 \cdot d \cdot r \cdot b$
      \item If total\_memory + $M$ $\leq C_{mem}$: allocate this rank
    \end{itemize}
    \item Validate: $\sum_{\ell=1}^{L} 2 \cdot d \cdot r_\ell \cdot b \leq C_{mem}$
  \end{enumerate}
  
  \vspace{0.3cm}
  
  \textbf{Example Result (2GB CPU):}
  \begin{itemize}
    \item Important layers (4-5): rank=8 each
    \item Less important (0-3): rank=4 each
    \item Total: 219KB $\ll$ 512MB budget 
  \end{itemize}
  
  \vspace{0.2cm}

\end{frame}

\begin{frame}{Phase 3: Split Federated Learning}
  \textbf{The Challenge:} Train without sharing data or full model
  
  \textbf{Why Split Learning?}
  \begin{itemize}
    \item \textbf{Privacy}: Client never sends raw data
    \item \textbf{Communication}: Send activations (1.5MB) not model (500MB)
    \item \textbf{Memory}: Client doesn't need top layers or task heads
  \end{itemize}
  
  \vspace{0.3cm}
  
  \textbf{Training Flow:}
  \begin{enumerate}
    \item \textbf{Client}: Forward through bottom 6 layers + LoRA $\rightarrow$ activations
    \item \textbf{Send}: Activations to server (with task\_id)
    \item \textbf{Server}: Forward through top 6 layers + task head $\rightarrow$ loss
    \item \textbf{Server}: Backward $\rightarrow$ activation gradients
    \item \textbf{Send}: Gradients back to client
    \item \textbf{Client}: Backward through LoRA only $\rightarrow$ update weights
  \end{enumerate}
  
  \vspace{0.2cm}
  
  Train only LoRA (12K params) not full layer (590K params)
\end{frame}

\begin{frame}{Phase 3: Per-Task Aggregation (HSplitLoRA)}
  \textbf{Why Not FedAvg?} Mixing sentiment + medical = worse than either
  
  \vspace{0.3cm}
  
  \textbf{Our Approach:} Aggregate within task groups only
  
  \vspace{0.3cm}
  
  \textbf{Key Benefits:}
  \begin{itemize}
    \item Base model remains frozen (no memory for gradients)
    \item Only LoRA adapters trained (0.1\% of parameters)
    \item Communication: activations during training, weights after
  \end{itemize}
\end{frame}

\begin{frame}{How Phases Integrate: The Complete Pipeline}
  \textbf{Phase 1 $\rightarrow$ Phase 3:} Task-aware aggregation
  \begin{itemize}
    \item Phase 1 outputs: \texttt{cluster\_labels = \{0:0, 1:0, 2:1\}}
    \item Phase 3 uses: Aggregate clients [0,1] separately from [2]
    \item \textbf{Result:} No task interference, better accuracy
  \end{itemize}
  
  \vspace{0.3cm}
  
  \textbf{Phase 2 $\rightarrow$ Phase 3:} Device-aware configuration
  \begin{itemize}
    \item Phase 2 outputs: \texttt{device\_profile, importance\_scores, rank\_config}
    \item Phase 3 uses: Auto-compute split point + LoRA ranks
    \item \textbf{Result:} 2GB CPU gets rank=4-8, 16GB GPU gets rank=16-32
  \end{itemize}
  
  \vspace{0.3cm}
  
  \textbf{Phase 1 + Phase 2 $\rightarrow$ Phase 3:} Optimal allocation
  \begin{itemize}
    \item High-variance cluster (diverse tasks) $\rightarrow$ higher ranks
    \item Low-variance cluster (similar tasks) $\rightarrow$ lower ranks
    \item \textbf{Result:} Resource allocation matches task complexity
  \end{itemize}
\end{frame}

\begin{frame}{Phase 4: MIRA Laplacian Regularization}
  \textbf{Personalized Model Updates:}
  
  \vspace{0.2cm}
  
  \begin{center}
  \begin{tikzpicture}[scale=1.0]
    % Before regularization
    \node[circle, draw, fill=blue!30, minimum size=1.1cm, font=\small] (w0) at (0,2.5) {$W_0$};
    \node[circle, draw, fill=blue!30, minimum size=1.1cm, font=\small] (w1) at (2.5,2.5) {$W_1$};
    \node[circle, draw, fill=red!30, minimum size=1.1cm, font=\small] (w2) at (6,2.5) {$W_2$};
    
    \node[above=0.3cm of w0] {\tiny Movie};
    \node[above=0.3cm of w1] {\tiny Product};
    \node[above=0.3cm of w2] {\tiny Medical};
    
    % After regularization
    \node[circle, draw, fill=blue!60, minimum size=1.1cm, font=\small] (w0p) at (0.5,0) {$W_0'$};
    \node[circle, draw, fill=blue!60, minimum size=1.1cm, font=\small] (w1p) at (2.0,0) {$W_1'$};
    \node[circle, draw, fill=red!30, minimum size=1.1cm, font=\small] (w2p) at (6,0) {$W_2'$};
    
    % Springs/connections
    \draw[thick, blue!70, line width=1.2pt] (w0) -- (w1);
    \draw[thick, blue!70, dashed, line width=1.2pt] (w0p) -- (w1p);
    
    % Arrows showing movement
    \draw[->, thick, orange, line width=1.5pt] (w0) -- (w0p);
    \draw[->, thick, orange, line width=1.5pt] (w1) -- (w1p);
    \draw[->, thick, gray, line width=1.2pt] (w2) -- (w2p);
    
    % Labels
    \node[right=0.5cm of w1p, text=blue, font=\small\bfseries] {Pulled closer};
    \node[right=0.3cm of w2p, text=red, font=\small\bfseries] {Unchanged};
    
    % Legend
    \node[below=0.5cm of w1p] {\tiny Before $\rightarrow$ After Regularization};
  \end{tikzpicture}
  \end{center}
  
  \vspace{0.3cm}
  
  \textbf{Formula:}
  \[
    W_k^{(t+1)} = W_k^{(t,R)} - \eta \sum_{\ell \in N_k} a_{k\ell}(W_k - W_\ell)
  \]
  
  \begin{itemize}
    \item $N_k$: Neighbors from task graph
    \item $a_{k\ell}$: Edge weight (similarity)
    \item $\eta$: Regularization strength
  \end{itemize}
\end{frame}

\begin{frame}{Laplacian Regularization: Intuition}
  \textbf{How It Works:}
  
  \vspace{0.3cm}
  
  \begin{columns}
    \column{0.5\textwidth}
    \textbf{Without Regularization:}
    \begin{itemize}
      \item Each client overfits to local data
      \item No knowledge sharing
      \item Poor generalization
    \end{itemize}
    
    \vspace{0.3cm}
    
    \textbf{Global Averaging (FedAvg):}
    \begin{itemize}
      \item All models merged into one
      \item Loses task specialization
      \item Movie + Medical = confused
    \end{itemize}
    
    \column{0.5\textwidth}
    \textbf{MIRA Regularization:}
    \begin{itemize}
      \item Similar tasks pull together
      \item Different tasks stay separate
      \item Each client keeps unique model
    \end{itemize}
    
    \vspace{0.3cm}
    
    \textbf{Result:}
    \begin{itemize}
      \item Personalized models
      \item Knowledge sharing within groups
      \item No harmful task mixing
    \end{itemize}
  \end{columns}

\end{frame}

% ============================================================================
% SECTION 3: MATHEMATICAL FOUNDATION
% ============================================================================
\section{Mathematical Foundation}

\begin{frame}{LoRA: Low-Rank Adaptation}
  \textbf{Core Concept:}
  
  \vspace{0.3cm}
  
  Instead of fine-tuning full weight matrix $W \in \mathbb{R}^{d_o \times d_i}$:
  
  \[
    W' = W + \Delta W, \quad \Delta W = B \cdot A^T
  \]
  
  where:
  \begin{itemize}
    \item $A \in \mathbb{R}^{r \times d_o}$, $B \in \mathbb{R}^{d_i \times r}$
    \item $r \ll d$ (e.g., $r=8$, $d=768$)
    \item Parameters: $2dr$ instead of $d^2$
  \end{itemize}
  
  \vspace{0.3cm}
  
  \textbf{Memory Model (fp32):}
  \[
    M_{param}(W, r) = 2 \cdot d \cdot r \cdot 4 \text{ bytes}
  \]
  
  \begin{itemize}
    \item Full layer: 768×768 = 589,824 params
    \item LoRA (r=8): 2×768×8 = 12,288 params
    \item \textbf{Reduction: 48×} fewer parameters!
  \end{itemize}
\end{frame}


% ============================================================================
% SECTION 4: IMPLEMENTATION STATUS
% ============================================================================
\section{Implementation Status}

\begin{frame}{Midterm Progress Status (50\% Complete)}
  \textbf{Task 1: Literature Review [100\% Complete]}
  \begin{itemize}
    \item Duration: Nov 17 - Dec 19, 2025 (5 weeks)
    \item Comprehensive review of FL, LoRA, Split Learning, MIRA
    \item Research gaps identified.
  \end{itemize}
  
  \vspace{0.2cm}
  
  \textbf{Task 2: Framework Design [50\% Complete - In Progress]}
  \begin{itemize}
    \item\textbf{Phase 1 Completed:} Task clustering (16 unit tests passing)
    \item\textbf{Phase 2 Completed:} LoRA configuration (20 unit tests passing)
    \item\textbf{Phase 3 In Progress:} Split learning (60\% - Week 3/6)
    \item\textbf{Phase 4 Planned:} Aggregation
  \end{itemize}
  
  \vspace{0.2cm}
  
  \textbf{Remaining Work:}
  \begin{itemize}
    \item Task 3: Testing \& Evaluation (Feb 2-13, 2026)
    \item Task 4: Documentation (Feb 16-20, 2026)
  \end{itemize}
\end{frame}

\begin{frame}{System Specifications}
  \textbf{Target Models for Final Testing:}
  \begin{itemize}
    \item GPT-2 (124M parameters)
    \item BERT (110M parameters)
    \item LLaMA-2 (13B, 70B parameters)
    \item LLaMA-3 (8B, 70B parameters)
    \item Falcon (7B, 40B parameters)
    \item Mistral (7B parameters)
  \end{itemize}
  
  \vspace{0.3cm}
  
  \textbf{Datasets:}
  \begin{itemize}
    \item \textbf{NLU:} GLUE, SuperGLUE (sentiment, NLI, reasoning)
    \item \textbf{QA:} SQuAD, Natural Questions, TriviaQA
    \item \textbf{Generation:} E2E NLG, CNN/DailyMail, XSum
    \item \textbf{Multilingual:} XNLI, mC4, FLORES-200
    \item \textbf{Domain-Specific:} MedQA (medical), LegalBench (legal), CodeSearchNet (code)
  \end{itemize}
    
\end{frame}


% ============================================================================
% SECTION 7: CONCLUSION
% ============================================================================
\section{Conclusion}

\begin{frame}{Summary}
  \textbf{ATLAS: Key Achievements}
  
  \vspace{0.5cm}
  
  \textbf{Technical Innovations:}
  \begin{enumerate}
    \item Gradient-based task clustering (Phase 1)
    \item Heterogeneous LoRA configuration (Phase 2)
    \item Split learning architecture (Phase 3)
    \item MIRA Laplacian regularization (Phase 4)
  \end{enumerate}
  
  \vspace{0.5cm}
  
  \textbf{Impact:} Enables personalized LLM fine-tuning on resource-constrained edge devices
\end{frame}

\begin{frame}[plain]
  \centering
  \Huge Thank You
  
  \vspace{1cm}
  
  \large Questions \& Discussion
  
  \vspace{0.5cm}
  
  \normalsize
  ATLAS: Adaptive Task-aware Federated Learning \\
  with LoRA-based Heterogeneous Splitting
\end{frame}

% ============================================================================
% APPENDIX
% ============================================================================
\appendix

\begin{frame}{Key Equations Reference}
  \textbf{LoRA Decomposition:}
  \[
    \Delta W = A B^T, \quad A \in \mathbb{R}^{d \times r}, \, B \in \mathbb{R}^{d \times r}
  \]
  
  \vspace{0.2cm}
  
  \textbf{Laplacian Regularization:}
  \[
    W_k^{(t+1)} = W_k^{(t,R)} - \eta \sum_{\ell \in N_k} a_{k\ell}(W_k - W_\ell)
  \]
  
  \vspace{0.2cm}
  
  \textbf{Graph Laplacian:}
  \[
    \mathcal{R}(W) = \frac{1}{2}\sum_{i,j} A_{ij} \|W_i - W_j\|^2 = W^T L W
  \]
  
  \vspace{0.2cm}
  
  \textbf{Communication Cost:}
  \[
    \text{Cost}_{\text{ATLAS}} = \mathcal{O}(r + h) \ll \mathcal{O}(d^2)
  \]
\end{frame}

\begin{frame}{Dataset Statistics}
  \begin{columns}
    \column{0.5\textwidth}
    \textbf{NLU Benchmarks:}
    \begin{table}
      \centering
      \tiny
      \begin{tabular}{lcc}
        \toprule
        Task & Examples & Type \\
        \midrule
        SST-2 & 67K & Sentiment \\
        RTE & 2.5K & NLI \\
        CoLA & 8.5K & Grammar \\
        BoolQ & 16K & QA \\
        WiC & 6K & WSD \\
        \bottomrule
      \end{tabular}
    \end{table}
    
    \vspace{0.2cm}
    
    \textbf{QA Datasets:}
    \begin{itemize}
      \item SQuAD: 100K pairs
      \item Natural Q: 307K
      \item TriviaQA: 650K
    \end{itemize}
    
    \column{0.5\textwidth}
    \textbf{Generation:}
    \begin{itemize}
      \item E2E NLG: 50K
      \item CNN/DM: 300K
      \item XSum: 227K
    \end{itemize}
    
    \vspace{0.2cm}
    
    \textbf{Multilingual:}
    \begin{itemize}
      \item XNLI: 393K (15 langs)
      \item mC4: Multi-TB (101 langs)
      \item FLORES: 3K (200 langs)
    \end{itemize}
    
    \vspace{0.2cm}
    
    \textbf{Domain-Specific:}
    \begin{itemize}
      \item MedQA: 61K medical
      \item LegalBench: 162 tasks
      \item CodeSearchNet: 2M code
    \end{itemize}
  \end{columns}
\end{frame}

\end{document}
